<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://bruceeezhao.github.io').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"always","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="1. Hadoop初识">
<meta name="keywords" content="大数据,Hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop">
<meta property="og:url" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1. Hadoop初识">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/Hadoop1vs2.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/hdfsarchitecture.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/yarn_architecture.gif">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/hdfsarchitecture.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/writeProcess.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/replicaSelection.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/datanodeDistance.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/readProcess.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/namenodeWork.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/datanodeWork.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/MapReduceCore.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/wordcountcase.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/FileInputFormat.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/CombineTextInputFormat.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/mapworkflow.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/reduceworkflow.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/ShuffleWorkFlow.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/MapTaskWorkFlow.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/ReduceTaskWorkFlow.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/join_table.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/compress_location.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/yarn_architecture.gif">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/yarnWorkFlow.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/fifo.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/capacity.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/fair.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/speculate.png">
<meta property="og:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/7_1.png">
<meta property="og:updated_time" content="2020-12-22T10:04:28.112Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hadoop">
<meta name="twitter:description" content="1. Hadoop初识">
<meta name="twitter:image" content="https://bruceeezhao.github.io/2020/11/05/Hadoop/Hadoop1vs2.png">

<link rel="canonical" href="https://bruceeezhao.github.io/2020/11/05/Hadoop/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Hadoop | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="default">
    <link itemprop="mainEntityOfPage" href="https://bruceeezhao.github.io/2020/11/05/Hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bruce zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hadoop
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-05 19:44:48" itemprop="dateCreated datePublished" datetime="2020-11-05T19:44:48+08:00">2020-11-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-22 18:04:28" itemprop="dateModified" datetime="2020-12-22T18:04:28+08:00">2020-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/Hadoop1vs2.png" alt></p><h1 id="1-Hadoop初识"><a href="#1-Hadoop初识" class="headerlink" title="1. Hadoop初识"></a>1. Hadoop初识</h1><a id="more"></a>
<h2 id="1-1-Hadoop1-x-与-2-x的区别"><a href="#1-1-Hadoop1-x-与-2-x的区别" class="headerlink" title="1.1. Hadoop1.x 与 2.x的区别"></a>1.1. Hadoop1.x 与 2.x的区别</h2><p>如首图所示，在1.x中MapReduce负责计算和资源调度，在2.x中，将资源调度的功能从MapReduce中分离出来，增加了Yarn模块。</p>
<h2 id="1-2-HDFS架构"><a href="#1-2-HDFS架构" class="headerlink" title="1.2. HDFS架构"></a>1.2. HDFS架构</h2><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener">HDFS文档</a></p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/hdfsarchitecture.png" alt></p>
<ol>
<li>NameNode: 存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等</li>
<li>DataNode: 在本地文件系统存储文件块数据，以及块数据的校验和</li>
<li>Secondary NameNode：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照</li>
</ol>
<h2 id="1-3-Yarn架构"><a href="#1-3-Yarn架构" class="headerlink" title="1.3. Yarn架构"></a>1.3. Yarn架构</h2><p><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Yarn文档</a></p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/yarn_architecture.gif" alt></p>
<ol>
<li><p>Resource Manager:的主要功能</p>
<ol>
<li>处理客户端请求</li>
<li>监控NodeManager</li>
<li>启动或监控ApplicationMaster</li>
<li>资源的分配与调度</li>
</ol>
</li>
<li><p>NodeManager:的主要功能</p>
<ol>
<li>管理单个节点上的资源</li>
<li>处理来自Resource Manager的命令</li>
<li>处理来自ApplicationMaster的命令</li>
</ol>
</li>
<li><p>ApplicationMaste:</p>
<ol>
<li>负责数据的切分</li>
<li>为应用程序申请资源与分配给内部的任务</li>
<li>任务的监控与容错</li>
</ol>
</li>
<li><p>Container</p>
<p>Container是Yarn中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。</p>
</li>
</ol>
<h2 id="1-4-MapReduce"><a href="#1-4-MapReduce" class="headerlink" title="1.4. MapReduce"></a>1.4. MapReduce</h2><p><a href="http://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html" target="_blank" rel="noopener">MapReduce文档</a></p>
<p>MapReduce将计算过程分为两个阶段：Map和Redece</p>
<p>1) Map阶段并行处理输入数据</p>
<p>2) Reduce阶段对Map结果进行汇总</p>
<h1 id="2-HDFS"><a href="#2-HDFS" class="headerlink" title="2. HDFS"></a>2. HDFS</h1><p>使用场景：适合一次写入，多次读出的场景，且不支持文件修改。适合用来做数据分析。</p>
<h2 id="2-1-优缺点"><a href="#2-1-优缺点" class="headerlink" title="2.1. 优缺点"></a>2.1. 优缺点</h2><h3 id="2-1-1-优点"><a href="#2-1-1-优点" class="headerlink" title="2.1.1. 优点"></a>2.1.1. 优点</h3><ol>
<li>高容错性<ol>
<li>数据自动保存多个副本，通过增加副本的形式提高容错性。</li>
<li>某一个副本丢失后，会自动创建新的副本，保证副本的数量</li>
</ol>
</li>
<li>适合处理大数据<ol>
<li>数据规模：能够处理GB,TB甚至PB级的数据</li>
<li>文件规模：能够处理百万规模以上的文件数量</li>
</ol>
</li>
<li>可在廉价机上构建</li>
</ol>
<h3 id="2-1-2-缺点"><a href="#2-1-2-缺点" class="headerlink" title="2.1.2. 缺点"></a>2.1.2. 缺点</h3><ol>
<li>不适合低延时数据访问，比如毫秒级</li>
<li>无法高效的对大量小文件进行存储<ol>
<li>存储小文件会占用NameNode大量的内存来存储文件目录和块信息。</li>
<li>小文件存储的寻址时间会超过读取时间</li>
</ol>
</li>
<li>不支持并发写入，文件随机修改<ol>
<li>一个文件只能有一个写，不允许多个线程同时写</li>
<li>仅支持数据追加（append），不支持文件随机修改</li>
</ol>
</li>
</ol>
<h2 id="2-2-组成架构"><a href="#2-2-组成架构" class="headerlink" title="2.2. 组成架构"></a>2.2. 组成架构</h2><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/hdfsarchitecture.png" alt></p>
<ol>
<li>NameNode: （master）<ol>
<li>管理HDFS的名称空间</li>
<li>配置副本策略</li>
<li>管理数据块（Block）映射信息</li>
<li>处理客户端读写请求</li>
</ol>
</li>
<li>DataNode: （slave）<ol>
<li>存储实际的数据块</li>
<li>执行数据块的读写操作</li>
</ol>
</li>
<li>client<ol>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成多个Block，然后上传。</li>
<li>与NameNode交互，获取文件的位置信息。</li>
<li>与DataNode交互，读取或写入数据。</li>
<li>Client提供一些命令管理HDFS</li>
<li>通过命令访问HDFS，如增删查改等</li>
</ol>
</li>
<li>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候并不能马上替换NameNode并提供服务<ol>
<li>复制NameNode，分担其工作，如定期合并Fsimage和Edits，并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复NameNode</li>
</ol>
</li>
</ol>
<h3 id="2-2-1-文件块大小（面试题）"><a href="#2-2-1-文件块大小（面试题）" class="headerlink" title="2.2.1. 文件块大小（面试题）"></a>2.2.1. 文件块大小（面试题）</h3><p>通过配置参数 <code>dfs.blocksize</code>来确定，默认大小在Hadoop2.x中是128M，老版本中是64M。这个大小是根据寻址时间和硬盘写入速度确定的，最佳状态是寻址时间是传输时间的1%。按寻址时间10ms，传输速度100M/s来计算，块大小需为100M。</p>
<p>块设置很小，就会增加寻址时间；如果太大，传输数据的时间会明显大于寻址时间。</p>
<h2 id="2-3-HDFS-API"><a href="#2-3-HDFS-API" class="headerlink" title="2.3. HDFS API"></a>2.3. HDFS API</h2><h3 id="2-3-1-环境配置"><a href="#2-3-1-环境配置" class="headerlink" title="2.3.1. 环境配置"></a>2.3.1. 环境配置</h3><ol>
<li><p>新建mvn工程</p>
</li>
<li><p>添加依赖</p>
<p>在<a href="https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client/2.9.2" target="_blank" rel="noopener">https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client/2.9.2</a> 选择与部署hadoop版本相同的mvn依赖，并添加依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>配置log4j</p>
<ol>
<li><p>新建log4j.properties文件</p>
</li>
<li><p>把下面的内容添加到文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">### 设置###</span><br><span class="line">log4j.rootLogger = info,stdout,D,E</span><br><span class="line"></span><br><span class="line">### 输出信息到控制抬 ###</span><br><span class="line">log4j.appender.stdout = org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target = System.out</span><br><span class="line">log4j.appender.stdout.layout = org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern = [%-5p] %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; method:%l%n%m%n</span><br><span class="line"></span><br><span class="line">### 输出DEBUG 级别以上的日志到=E://logs/error.log ###</span><br><span class="line">log4j.appender.D = org.apache.log4j.DailyRollingFileAppender</span><br><span class="line">log4j.appender.D.File = ./logs/log.log</span><br><span class="line">log4j.appender.D.Append = true</span><br><span class="line">log4j.appender.D.Threshold = DEBUG</span><br><span class="line">log4j.appender.D.layout = org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.D.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125;  [ %t:%r ] - [ %p ]  %m%n</span><br><span class="line"></span><br><span class="line">### 输出ERROR 级别以上的日志到=E://logs/error.log ###</span><br><span class="line">log4j.appender.E = org.apache.log4j.DailyRollingFileAppender</span><br><span class="line">log4j.appender.E.File =./logs/error.log</span><br><span class="line">log4j.appender.E.Append = true</span><br><span class="line">log4j.appender.E.Threshold = ERROR</span><br><span class="line">log4j.appender.E.layout = org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.E.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125;  [ %t:%r ] - [ %p ]  %m%n</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>测试</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSClient</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        HDFSClient h = <span class="keyword">new</span> HDFSClient();</span><br><span class="line">        h.test();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 0 一个简单的测试</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 1 获取hdfs客户端对象</span></span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2 在hdfs上创建路径</span></span><br><span class="line">            fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/1109/dashi"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3 关闭资源</span></span><br><span class="line">            fs.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"over"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - bruce supergroup          0 2020-11-10 10:56 /1109</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="2-3-2-上传文件"><a href="#2-3-2-上传文件" class="headerlink" title="2.3.2. 上传文件"></a>2.3.2. 上传文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 文件上传</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLoaclFile</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 获取fs对象</span></span><br><span class="line">       Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">       FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 执行上传api</span></span><br><span class="line">           fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"/home/bruce/Desktop/hadooptest.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/hadooptest.txt"</span>));</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 关闭资源</span></span><br><span class="line">           fs.close();</span><br><span class="line">       &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - bruce supergroup          0 2020-11-10 10:56 /1109</span><br><span class="line">-rw-r--r--   3 bruce supergroup         97 2020-11-10 11:15 /hadooptest.txt</span><br></pre></td></tr></table></figure>

<h3 id="2-3-3-下载文件"><a href="#2-3-3-下载文件" class="headerlink" title="2.3.3. 下载文件"></a>2.3.3. 下载文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2 文件下载</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 获取fs对象</span></span><br><span class="line">       Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">       FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 执行下载api</span></span><br><span class="line">           fs.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/hadooptest.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"./hadoop.txt"</span>));</span><br><span class="line">           <span class="comment">// 关闭资源</span></span><br><span class="line">           fs.close();</span><br><span class="line">       &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-3-4-文件删除"><a href="#2-3-4-文件删除" class="headerlink" title="2.3.4. 文件删除"></a>2.3.4. 文件删除</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3 文件删除</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取fs对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 执行删除api</span></span><br><span class="line">            fs.delete(<span class="keyword">new</span> Path(<span class="string">"/hadooptest.txt"</span>));</span><br><span class="line">            <span class="comment">// 关闭资源</span></span><br><span class="line">            fs.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-3-5-文件信息打印"><a href="#2-3-5-文件信息打印" class="headerlink" title="2.3.5. 文件信息打印"></a>2.3.5. 文件信息打印</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 4 查看文件信息</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取fs对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 查看文件信息</span></span><br><span class="line">            RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line">            <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">                LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line">                System.out.println(fileStatus.getPath().getName());</span><br><span class="line">                System.out.println(fileStatus.getPermission());</span><br><span class="line">                System.out.println(fileStatus.getLen());</span><br><span class="line"></span><br><span class="line">                BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">                <span class="keyword">for</span> (BlockLocation block: blockLocations</span><br><span class="line">                     ) &#123;</span><br><span class="line">                    String[] hosts = block.getHosts();</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> (String host: hosts) &#123;</span><br><span class="line">                        System.out.println(host);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"===================="</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 关闭资源</span></span><br><span class="line">            fs.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-4-HDFS-I-O流操作"><a href="#2-4-HDFS-I-O流操作" class="headerlink" title="2.4. HDFS I/O流操作"></a>2.4. HDFS I/O流操作</h2><h3 id="2-4-1-上传文件"><a href="#2-4-1-上传文件" class="headerlink" title="2.4.1. 上传文件"></a>2.4.1. 上传文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 上传文件</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpLoadFile</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 1 获取对象</span></span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2 获取输入流</span></span><br><span class="line">            FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"/home/bruce/Desktop/hadooptest.txt"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3 获取输出流</span></span><br><span class="line">            FSDataOutputStream fos = fs.create(<span class="keyword">new</span> Path(<span class="string">"/zhangsan.txt"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 4 流的对烤</span></span><br><span class="line">            IOUtils.copyBytes(fis, fos, conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 5 关闭资源</span></span><br><span class="line">            IOUtils.closeStream(fis);</span><br><span class="line">            IOUtils.closeStream(fos);</span><br><span class="line">            fs.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-4-2-下载文件"><a href="#2-4-2-下载文件" class="headerlink" title="2.4.2. 下载文件"></a>2.4.2. 下载文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载文件</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDownFile</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 1 获取对象</span></span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2 获取输入流</span></span><br><span class="line">            FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/zhangsan.txt"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3 获取输出流</span></span><br><span class="line">            FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"./lisi.txt"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 4 流的对烤</span></span><br><span class="line">            IOUtils.copyBytes(fis, fos, conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 5 关闭资源</span></span><br><span class="line">            IOUtils.closeStream(fis);</span><br><span class="line">            IOUtils.closeStream(fos);</span><br><span class="line">            fs.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-4-3-读取部分文件"><a href="#2-4-3-读取部分文件" class="headerlink" title="2.4.3. 读取部分文件"></a>2.4.3. 读取部分文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取部分内容</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 1 获取对象</span></span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2 获取输入流</span></span><br><span class="line">            FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/zhangsan.txt"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3 获取输出流</span></span><br><span class="line">            FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"./wangwu.txt"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 4 流的对烤</span></span><br><span class="line">            <span class="keyword">byte</span>[] bytes = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;<span class="number">1</span> ; i++) &#123;</span><br><span class="line">                <span class="keyword">int</span> len = fis.read(bytes,<span class="number">0</span>, <span class="number">5</span>);</span><br><span class="line">                fos.write(bytes,<span class="number">0</span>, <span class="number">5</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 5 关闭资源</span></span><br><span class="line">            fis.close();</span><br><span class="line">            fos.close();</span><br><span class="line">            fs.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-5-HDFS数据流"><a href="#2-5-HDFS数据流" class="headerlink" title="2.5. HDFS数据流"></a>2.5. HDFS数据流</h2><h3 id="2-5-1-写入流程"><a href="#2-5-1-写入流程" class="headerlink" title="2.5.1. 写入流程"></a>2.5.1. 写入流程</h3><h4 id="2-5-1-1-数据写入"><a href="#2-5-1-1-数据写入" class="headerlink" title="2.5.1.1. 数据写入"></a>2.5.1.1. 数据写入</h4><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/writeProcess.png" alt></p>
<h4 id="2-5-1-2-副本存储节点选择"><a href="#2-5-1-2-副本存储节点选择" class="headerlink" title="2.5.1.2. 副本存储节点选择"></a>2.5.1.2. 副本存储节点选择</h4><p>文档版本 - 3.2.1</p>
<p>在副本数量为3的情况下：</p>
<ol>
<li>如果writer在一个datanode上，那么第一个副本就存储在这个datanode上，否则选择与writer在同一个机架的随机节点。</li>
<li>第二个节点选择在不同机架的一个datanode</li>
<li>第三个节点选择在不同机架上的与第二个节点不同的datanode</li>
</ol>
<p>如果复制因子大于3，则随机决定第4个副本和后面的副本的位置，同时保持每个机架的副本数量低于上限(基本上是(副本数 - 1) /机架+ 2)。</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/replicaSelection.png" alt></p>
<p>因为NameNode不允许数据节点拥有同一个块的多个副本，所以创建的副本的最大数量是当前数据节点的总数。</p>
<h4 id="2-5-1-3-节点距离的计算"><a href="#2-5-1-3-节点距离的计算" class="headerlink" title="2.5.1.3. 节点距离的计算"></a>2.5.1.3. 节点距离的计算</h4><p>在HDFS读写数据的过程中，NameNode会选择距离待上传/下载数据最近距离的Datanode。</p>
<p>节点距离 = 两个节点到达最近共同祖先的距离总和。</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/datanodeDistance.png" alt></p>
<p>distance(00, 00) = 0  (同一节点上的进程)</p>
<p>distance(00, 01) = 2  (同一机架上的不同节点) </p>
<p>distance(00, 05) = 4  (同一数据中心，不同机架上的节点)</p>
<p>distance(00, 15) = 6  (不同数据中心的节点)</p>
<h3 id="2-5-2-读取流程"><a href="#2-5-2-读取流程" class="headerlink" title="2.5.2. 读取流程"></a>2.5.2. 读取流程</h3><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/readProcess.png" alt></p>
<h2 id="2-6-NameNode和Secondary-NameNode"><a href="#2-6-NameNode和Secondary-NameNode" class="headerlink" title="2.6. NameNode和Secondary NameNode"></a>2.6. NameNode和Secondary NameNode</h2><h3 id="2-6-1-工作机制"><a href="#2-6-1-工作机制" class="headerlink" title="2.6.1. 工作机制"></a>2.6.1. 工作机制</h3><ol>
<li><p>NameNode的元数据存放在内存中，为了防止断电丢失，在磁盘中存一个备份FsImage。</p>
</li>
<li><p>这样带来新的问题，如果在内存中更新的同时更新fsimage，就会导致效率过低，如果不更新，就会发生一致性问题，一旦NameNode断电，就会产生数据丢失。因此，引入<strong>Edits</strong>文件（只进行追加操作，效率很高）。每当元数据有更新或添加元数据时，修改内存中的元数据并追加到Edits中（先更新Edits再更新内存）。这样，一旦NameNode断电，可以通过fsimage和Edits的合并，合成元数据。</p>
</li>
<li><p>如果长时间添加数据到Edits中，会导致文件数据过大，效率降低，一旦断电，恢复元数据需要的时间很长。因此需要定期进行fsimage和Edits的合并。合并操作由Secondary NameNode完成。</p>
</li>
</ol>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/namenodeWork.png" alt></p>
<p><strong>tips</strong>[hdfs-default.xml]</p>
<ol>
<li>定时时间默认3600秒</li>
<li>edits中的数据大于100万条，每隔60秒检查一次。</li>
</ol>
<h3 id="2-6-2-Fsimage-和-Edits"><a href="#2-6-2-Fsimage-和-Edits" class="headerlink" title="2.6.2. Fsimage 和 Edits"></a>2.6.2. Fsimage 和 Edits</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/tmp/hadoop-bruce/dfs/name/current$ ls</span><br><span class="line">edits_0000000000000000001-0000000000000000002  fsimage_0000000000000000004</span><br><span class="line">edits_0000000000000000003-0000000000000000004  fsimage_0000000000000000004.md5</span><br><span class="line">edits_inprogress_0000000000000000005           seen_txid</span><br><span class="line">fsimage_0000000000000000002                    VERSION</span><br><span class="line">fsimage_0000000000000000002.md5</span><br></pre></td></tr></table></figure>

<ol>
<li>fsimage文件：HDFS文件系统元数据的一个永久性检查点，其中包括HDFS文件系统的所有目录和文件idnode的序列化信息。</li>
<li>edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中</li>
<li>seen_txid文件：保存的是一个数字，就是最后一个edits的数字</li>
<li>每次namenode启动时都会将fsimage文件读入内存，加载edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成namenode启动的时候就将fsiamge和edits文件进行了合并。</li>
</ol>
<p>在hdfs中新建目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /zhao</span><br></pre></td></tr></table></figure>

<p>查看操作日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/tmp/hadoop-bruce/dfs/name/current$ hdfs oev -p XML -i edits_inprogress_0000000000000000005 -o ed.xml</span><br><span class="line">/tmp/hadoop-bruce/dfs/name/current$ cat ed.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>5<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_MKDIR<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>6<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/zhao<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1605182998565<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>bruce<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>493<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">EDITS</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-6-3-NameNode多目录配置"><a href="#2-6-3-NameNode多目录配置" class="headerlink" title="2.6.3. NameNode多目录配置"></a>2.6.3. NameNode多目录配置</h3><p>修改hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>作用：</p>
<p>相当于给Namenode增加了备份</p>
<h2 id="2-7-Datanode（面试重点）"><a href="#2-7-Datanode（面试重点）" class="headerlink" title="2.7. Datanode（面试重点）"></a>2.7. Datanode（面试重点）</h2><h3 id="2-7-1-工作机制"><a href="#2-7-1-工作机制" class="headerlink" title="2.7.1. 工作机制"></a>2.7.1. 工作机制</h3><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/datanodeWork.png" alt></p>
<ol>
<li>一个数据块在Datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳</li>
<li>Datanode启动后向NameNode注册，通过后，周期性（1小时）的向Namenode上报所有块的信息</li>
<li>心跳3秒一次，心跳返回结果带有Namenode给该Datanode的命令，如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个Datanode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入或退出一些机器</li>
</ol>
<h3 id="2-7-2-数据完整性"><a href="#2-7-2-数据完整性" class="headerlink" title="2.7.2. 数据完整性"></a>2.7.2. 数据完整性</h3><ol>
<li>当Datanode读取Block的时候，它会计算CheckSum</li>
<li>如果计算后的CheckSum，与创建时值不一样，说明Block已经损坏</li>
<li>Client读取其他Datanode上的Block</li>
<li>Datanode在其文件创建后周期验证CheckSum</li>
</ol>
<h3 id="2-7-3-白名单-amp-黑名单"><a href="#2-7-3-白名单-amp-黑名单" class="headerlink" title="2.7.3. 白名单&amp;黑名单"></a>2.7.3. 白名单&amp;黑名单</h3><ol>
<li><p>白名单</p>
<p>白名单中添加的节点可以被Namenode管理。</p>
<p>操作步骤：</p>
<p>1) 新建hosts文件  dfs.hosts，并在文件中添加允许的主机名</p>
<p>2) 在hdfs-site.xml中添加配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>dfs.hots path<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3) 配置分发</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ xsync hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p>4) 刷新Namenode</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfsadmin -refreashNodes</span><br></pre></td></tr></table></figure>

<p>5) 更新ResourceManager节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yarn rmadmin -refreashNodes</span><br></pre></td></tr></table></figure>

<p>6) 如果数据不平衡，可令再平衡</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-balancer.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>黑名单</p>
<p>黑名单中的主机会被强制下线。</p>
<p>操作步骤：</p>
<p>1) 创建黑名单文件 dfs.hosts.exclude ，并添加主机名</p>
<p>2) 在hdfs-site.xml 中添加配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>dfs.hots.exclude path<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3) 配置分发</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ xsync hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p>4) 刷新Namenode</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfsadmin -refreashNodes</span><br></pre></td></tr></table></figure>

<p>5) 更新ResourceManager节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yarn rmadmin -refreashNodes</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="2-7-4-Datanode多目录配置"><a href="#2-7-4-Datanode多目录配置" class="headerlink" title="2.7.4. Datanode多目录配置"></a>2.7.4. Datanode多目录配置</h3><p>修改hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>作用：</p>
<p>将数据分开存储，例如第一个文件存储在data1，第二个文件存储在data2</p>
<h2 id="2-8-HDFS-2-X新特性"><a href="#2-8-HDFS-2-X新特性" class="headerlink" title="2.8. HDFS 2.X新特性"></a>2.8. HDFS 2.X新特性</h2><h3 id="2-8-1-集群将数据拷贝"><a href="#2-8-1-集群将数据拷贝" class="headerlink" title="2.8.1. 集群将数据拷贝"></a>2.8.1. 集群将数据拷贝</h3><p>distcp命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop distcp hdfs://hadoop102:9000/user/hello.txt hdfs://hadoop103:9000/user/hello.txt</span><br></pre></td></tr></table></figure>

<h3 id="2-8-2-小文件存档"><a href="#2-8-2-小文件存档" class="headerlink" title="2.8.2 小文件存档"></a>2.8.2 小文件存档</h3><p>每个文件均按块存储，每个块的元数据存储在Namenode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode的内存。但是存储小文件所需的磁盘容量与数据块大小无关，例如数据块的大小为128M，文件大小为1M，那么实际的存储是使用1M的磁盘空间。</p>
<ol>
<li><p>解决方法之一</p>
<p>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块中，在减少NameNode内存使用的同时，允许对文件进行透明访问。具体来说，HDFS存档文件对内是一个一个的独立文件，对NameNode而言却是一个整体，减少了NameNode的内存</p>
</li>
</ol>
<p>归档：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop archive -archiveName NAME.har -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt;</span><br></pre></td></tr></table></figure>

<p>查看归档：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls -R har:///&lt;dest&gt;</span><br></pre></td></tr></table></figure>

<h3 id="2-8-3-回收站"><a href="#2-8-3-回收站" class="headerlink" title="2.8.3. 回收站"></a>2.8.3. 回收站</h3><ol>
<li>fs.trash.interval=0， 0表示禁用回收站，其他值表示文件的存活时间</li>
<li>fs.trash.checkpoint.interval=0，检查回收站的间隔时间。0表示与fs.trash.interval的值相等。</li>
<li>fs.trash.checkpoint.interval &lt;= fs.trash.interval</li>
</ol>
<h3 id="2-8-4-快照"><a href="#2-8-4-快照" class="headerlink" title="2.8.4. 快照"></a>2.8.4. 快照</h3><p>快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写发生时，才会产生新文件。</p>
<h1 id="3-MapReduce"><a href="#3-MapReduce" class="headerlink" title="3. MapReduce"></a>3. MapReduce</h1><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li>MrAppMaster: 负责整个程序的过程调度及状态协调</li>
<li>MapTask: 负责Map阶段的整个数据处理流程</li>
<li>ReduceTask: 负责Reduce阶段的整个数据处理流程</li>
</ol>
<h2 id="3-1-优缺点"><a href="#3-1-优缺点" class="headerlink" title="3.1. 优缺点"></a>3.1. 优缺点</h2><ol>
<li>优点<ol>
<li>易于编程，容易编写分布式程序</li>
<li>良好的扩展性，增加机器可增加计算能力</li>
<li>高容错性，一台机器挂了hadoop会将任务转移到另一个节点</li>
<li>适合PB级以上数据的离线运算</li>
</ol>
</li>
<li>缺点<ol>
<li>不擅长实时计算</li>
<li>不擅长流式计算</li>
<li>不擅长有向图(DAG)计算</li>
</ol>
</li>
</ol>
<h2 id="3-2-编程思想"><a href="#3-2-编程思想" class="headerlink" title="3.2. 编程思想"></a>3.2. 编程思想</h2><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/MapReduceCore.png" alt></p>
<h2 id="3-3-编程规范"><a href="#3-3-编程规范" class="headerlink" title="3.3. 编程规范"></a>3.3. 编程规范</h2><p>用户编写的程序分成三个部分： Mapper、Reducer、Driver</p>
<ol>
<li><p>Mapper阶段</p>
<ol>
<li>用户自定义的Mapper要继承父类</li>
<li>Mapper的输入数据是KV对的形式(KV的类型可自定义)</li>
<li>Mapper中的业务逻辑写在map()方法中</li>
<li>Mapper的输出数据是KV对的形式(KV的类型可自定义)</li>
<li>map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次</li>
</ol>
</li>
<li><p>Reducer阶段</p>
<ol>
<li>用户自定义的Reducer要继承父类</li>
<li>Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</li>
<li>Reducer的业务逻辑写在reduce()方法中</li>
<li>ReduceTask进程对每一组相同K的&lt;K,V&gt;组调用一次reduce()方法</li>
</ol>
</li>
<li><p>Driver阶段</p>
<p>相当于Yarn集群的客户端，用于提交我们的程序到Yarn集群，提交的是封装了MapReduce程序相关运行参数的job对象。</p>
</li>
</ol>
<h2 id="3-4-WordCount-案例"><a href="#3-4-WordCount-案例" class="headerlink" title="3.4. WordCount 案例"></a>3.4. WordCount 案例</h2><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/wordcountcase.png" alt></p>
<ol>
<li><p>Mapper</p>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/blob/master/Hadoop/hdfs/src/main/java/mapreduce/wordcount/WordCountMapper.java" target="_blank" rel="noopener">代码路径</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * map阶段</span></span><br><span class="line"><span class="comment"> * KEYIN 输入数据的Key</span></span><br><span class="line"><span class="comment"> * VALUEIN 输入数据的value</span></span><br><span class="line"><span class="comment"> * KEYOUT 输出数据的key， 同时也是Reduce阶段的输入</span></span><br><span class="line"><span class="comment"> * VALUEOUT 输出数据的value， 同时也是Reduce阶段的输入</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 2 切割单词</span></span><br><span class="line">        String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 3 循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word: words) &#123;</span><br><span class="line">            k.set(word);</span><br><span class="line"></span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reducer</p>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/blob/master/Hadoop/hdfs/src/main/java/mapreduce/wordcount/WordCountReducer.java" target="_blank" rel="noopener">代码路径</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value: values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver</p>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/blob/master/Hadoop/hdfs/src/main/java/mapreduce/wordcount/WordCountDriver.java" target="_blank" rel="noopener">代码路径</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        <span class="comment">// 1 获取Job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar存储位置</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联Map和Reduce类</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置Mapper阶段输出数据的key，value类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key，value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="3-5-序列化"><a href="#3-5-序列化" class="headerlink" title="3.5. 序列化"></a>3.5. 序列化</h2><h3 id="3-5-1-定义"><a href="#3-5-1-定义" class="headerlink" title="3.5.1. 定义"></a>3.5.1. 定义</h3><ol>
<li><p>序列化</p>
<p>把内存中的对象转换成字节序列（或其他数据传输协议）以便存储到磁盘（持久化）和网络传输</p>
</li>
<li><p>反序列化</p>
<p>将接收到的字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象</p>
</li>
</ol>
<h3 id="3-5-2-为什么不用Java的序列化"><a href="#3-5-2-为什么不用Java的序列化" class="headerlink" title="3.5.2. 为什么不用Java的序列化"></a>3.5.2. 为什么不用Java的序列化</h3><p>Java的序列化是一个重量级序列化框架(Serializable)，一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以Hadoop自己开发了一套序列化机制（Writeable）。</p>
<p><strong>Hadoop 序列化特点：</strong></p>
<ol>
<li>紧凑：高效使用存储空间</li>
<li>快速：读写数据的额外开销小</li>
<li>可扩展：随着通信协议的升级可升级</li>
<li>互操作：支持多种语言的交互</li>
</ol>
<h3 id="3-5-3-序列化流程"><a href="#3-5-3-序列化流程" class="headerlink" title="3.5.3. 序列化流程"></a>3.5.3. 序列化流程</h3><p>自定义bean对象实现序列化</p>
<p>步骤：</p>
<ol>
<li><p>实现Writeable接口</p>
</li>
<li><p>提供空参构造函数</p>
</li>
<li><p>重写序列化方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeLong(up);</span><br><span class="line">        dataOutput.writeLong(down);</span><br><span class="line">        dataOutput.writeLong(sum);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重写反序列化方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 反序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 必须和序列化的顺序一致</span></span><br><span class="line">        up = dataInput.readLong();</span><br><span class="line">        down = dataInput.readLong();</span><br><span class="line">        sum = dataInput.readLong();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>反序列化与序列化的顺序必须一致</p>
</li>
<li><p>想要把结果显示在文件中，需要重写toString方法</p>
</li>
<li><p>如果需要将自定义的bean放在key中传输，还需要实现Comparable接口，实现compareTo方法，因为MapReduce框架中的Shuffle过程要求key必须可排序。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FLowBean o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.sum &gt; o.getSum() ? -<span class="number">1</span>: <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="3-5-4-案例"><a href="#3-5-4-案例" class="headerlink" title="3.5.4. 案例"></a>3.5.4. 案例</h3><p><a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/serial" target="_blank" rel="noopener">完整代码路径</a></p>
<ol>
<li><p>实现bean</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce.serial;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FLowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 上行流量</span></span><br><span class="line">    <span class="keyword">private</span> Long up;</span><br><span class="line">    <span class="comment">// 下行流量</span></span><br><span class="line">    <span class="keyword">private</span> Long down;</span><br><span class="line">    <span class="comment">// 总流量</span></span><br><span class="line">    <span class="keyword">private</span> Long sum;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空参构造，为了后续反射用</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FLowBean</span><span class="params">()</span> </span>&#123; <span class="keyword">super</span>();&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FLowBean</span><span class="params">(Long up, Long down)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.up = up;</span><br><span class="line">        <span class="keyword">this</span>.down = down;</span><br><span class="line">        sum = up + down;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeLong(up);</span><br><span class="line">        dataOutput.writeLong(down);</span><br><span class="line">        dataOutput.writeLong(sum);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 必须和序列化的顺序一致</span></span><br><span class="line">        up = dataInput.readLong();</span><br><span class="line">        down = dataInput.readLong();</span><br><span class="line">        sum = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> up + <span class="string">" "</span> + down + <span class="string">" "</span> + sum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> up;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUp</span><span class="params">(Long up)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.up = up;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDown</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> down;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDown</span><span class="params">(Long down)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.down = down;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSum</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSum</span><span class="params">(Long sum)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sum = sum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(Long up, Long down)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.up = up;</span><br><span class="line">        <span class="keyword">this</span>.down = down;</span><br><span class="line">        sum = up + down;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="2">
<li><p>实现Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce.serial;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBeanMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FLowBean</span>&gt; </span>&#123;</span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    FLowBean v = <span class="keyword">new</span> FLowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 切割</span></span><br><span class="line">        String[] items = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装</span></span><br><span class="line">        k.set(items[<span class="number">0</span>]);</span><br><span class="line">        v.set(Long.parseLong(items[<span class="number">1</span>]), Long.parseLong(items[<span class="number">2</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(k, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现Reducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce.serial;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FLowBean</span>, <span class="title">Text</span>, <span class="title">FLowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FLowBean fLowBean = <span class="keyword">new</span> FLowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FLowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> sumup = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> sumdown = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (FLowBean v: values</span><br><span class="line">             ) &#123;</span><br><span class="line">            sumup += v.getUp();</span><br><span class="line">            sumdown += v.getDown();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        fLowBean.set(sumup, sumdown);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(key, fLowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapreduce.serial;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        <span class="comment">// 1 获取job</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar路径</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 设置mapper和reducer</span></span><br><span class="line">        job.setMapperClass(FlowBeanMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置mapper输出类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FLowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FLowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/phonedata"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/output"</span>));</span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="3-6-MapReduce框架原理"><a href="#3-6-MapReduce框架原理" class="headerlink" title="3.6. MapReduce框架原理"></a>3.6. MapReduce框架原理</h2><h3 id="3-6-1-InputFormat数据输入"><a href="#3-6-1-InputFormat数据输入" class="headerlink" title="3.6.1. InputFormat数据输入"></a>3.6.1. InputFormat数据输入</h3><h4 id="3-6-1-1-切片与MapTask并行度决定机制"><a href="#3-6-1-1-切片与MapTask并行度决定机制" class="headerlink" title="3.6.1.1. 切片与MapTask并行度决定机制"></a>3.6.1.1. 切片与MapTask并行度决定机制</h4><p>MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。</p>
<ol>
<li><p>MapTask并行度决定机制</p>
<p>名词解释：</p>
<ol>
<li>数据块： Block是HDFS物理上把数据分成一块一块</li>
<li>数据切片：只是在逻辑上对输入进行分片，并不会在磁盘上将其切片存储</li>
</ol>
</li>
</ol>
<p>1）一个Job的Map阶段并行度是由客户端在提交Job时的切片数决定的</p>
<p>2）每一个切片分配一个MapTask并行实例处理</p>
<p>3）默认情况下切片大小为块大小BlockSize</p>
<h4 id="3-6-1-2-Job提交流程和切片源码"><a href="#3-6-1-2-Job提交流程和切片源码" class="headerlink" title="3.6.1.2. Job提交流程和切片源码"></a>3.6.1.2. Job提交流程和切片源码</h4><p>Job提交</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

<p>切片源码：</p>
<p>[FileInputFormat.class]</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        StopWatch sw = (<span class="keyword">new</span> StopWatch()).start();</span><br><span class="line">        <span class="keyword">long</span> minSize = Math.max(<span class="keyword">this</span>.getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">        <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line">        List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList();</span><br><span class="line">        List&lt;FileStatus&gt; files = <span class="keyword">this</span>.listStatus(job);</span><br><span class="line">        Iterator i$ = files.iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">                <span class="keyword">while</span>(i$.hasNext()) &#123;</span><br><span class="line">                    FileStatus file = (FileStatus)i$.next();</span><br><span class="line">                    Path path = file.getPath();</span><br><span class="line">                    <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">                    <span class="keyword">if</span> (length != <span class="number">0L</span>) &#123;</span><br><span class="line">                        BlockLocation[] blkLocations;</span><br><span class="line">                        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">                            blkLocations = ((LocatedFileStatus)file).getBlockLocations();</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">                            blkLocations = fs.getFileBlockLocations(file, <span class="number">0L</span>, length);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">this</span>.isSplitable(job, path)) &#123;</span><br><span class="line">                            <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">                            <span class="keyword">long</span> splitSize = <span class="keyword">this</span>.computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">long</span> bytesRemaining;</span><br><span class="line">                            <span class="keyword">int</span> blkIndex;</span><br><span class="line">                            <span class="keyword">for</span>(bytesRemaining = length; (<span class="keyword">double</span>)bytesRemaining / (<span class="keyword">double</span>)splitSize &gt; <span class="number">1.1</span>D; bytesRemaining -= splitSize) &#123;</span><br><span class="line">                                blkIndex = <span class="keyword">this</span>.getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">                                splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> (bytesRemaining != <span class="number">0L</span>) &#123;</span><br><span class="line">                                blkIndex = <span class="keyword">this</span>.getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">                                splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="keyword">if</span> (LOG.isDebugEnabled() &amp;&amp; length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span><br><span class="line">                                LOG.debug(<span class="string">"File is not splittable so no parallelization is possible: "</span> + file.getPath());</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            splits.add(<span class="keyword">this</span>.makeSplit(path, <span class="number">0L</span>, length, blkLocations[<span class="number">0</span>].getHosts(), blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        splits.add(<span class="keyword">this</span>.makeSplit(path, <span class="number">0L</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                job.getConfiguration().setLong(<span class="string">"mapreduce.input.fileinputformat.numinputfiles"</span>, (<span class="keyword">long</span>)files.size());</span><br><span class="line">                sw.stop();</span><br><span class="line">                <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">                    LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size() + <span class="string">", TimeTaken: "</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> splits;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/FileInputFormat.png" alt></p>
<h4 id="3-6-1-3-CombineTextInputFormat-切片"><a href="#3-6-1-3-CombineTextInputFormat-切片" class="headerlink" title="3.6.1.3. CombineTextInputFormat 切片"></a>3.6.1.3. CombineTextInputFormat 切片</h4><ol>
<li><p>产生原因</p>
<p>框架默认的TextInputFormat切片机制是对任务按文件进行切片，不管文件多小，都会是一个单独的切片，在有大量的小文件的情况下，就会产生大量的MapTask，效率低下。</p>
<p>因此引入CombineTextInputFormat，来处理小文件过多的情况，它可以将多个小文件从逻辑上划分到一个切片中，这样多个小文件就交给一个MapTask处理。</p>
</li>
<li><p>虚拟存储切片最大值设置</p>
<p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);</p>
</li>
<li><p>切片机制</p>
<p>生成切片包括两部分： 虚拟存储过程和切片过程</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/CombineTextInputFormat.png" alt></p>
</li>
</ol>
<p>如何使用：</p>
<p>需要在Driver中添加CombineTextInputFormat的配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">20971520</span>);</span><br></pre></td></tr></table></figure>

<h4 id="3-6-1-4-FileInputFormat实现类"><a href="#3-6-1-4-FileInputFormat实现类" class="headerlink" title="3.6.1.4. FileInputFormat实现类"></a>3.6.1.4. FileInputFormat实现类</h4><ol>
<li><p>TextInputFormat</p>
<p>是默认的FileInputFormat实现类。按行读取记录。键是存储改行在整个文件中的起始字节偏移量，LongWritable类型。值是这行的内容，不包括终止符，Text类型。</p>
</li>
<li><p>KeyValueTextInputFormat</p>
<p>每一行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">"\t"</span>);</span><br></pre></td></tr></table></figure>

<p>来设置分隔符。默认分隔符是\t</p>
<p>之后修改使用的Format类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setInputFormatClass(KeyValueTextInputFormat.class);</span><br></pre></td></tr></table></figure>
</li>
<li><p>NLineInputFormat</p>
<p>如果使用NLineInputFormat，代表每个map进程处理的InputSplit不再按照Block块划分，而是按NLineInputFormat指定的行数N来划分。即输入文件的总行数/N=切片数，如果不整除，切片数=商+1。</p>
<p>示例： 有以下4行数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rich learning from</span><br><span class="line">hello world</span><br><span class="line">come from america</span><br><span class="line">welcome</span><br></pre></td></tr></table></figure>

<p>如果设置N为2，则每个分片包括2行。开启两个MapTask</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(0, Rich learning from)</span><br><span class="line">(19, hello world)</span><br></pre></td></tr></table></figure>

<p>另一个</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(30, come from america)</span><br><span class="line">(49, welcome)</span><br></pre></td></tr></table></figure>

<p>使用： 修改驱动类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">job.setInputFormatClass(NLineInputFormat.class);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="3-6-1-5-自定义InputFormat"><a href="#3-6-1-5-自定义InputFormat" class="headerlink" title="3.6.1.5. 自定义InputFormat"></a>3.6.1.5. 自定义InputFormat</h4><ol>
<li><p>案例：</p>
<ol>
<li><p>需求：</p>
<p>将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value的文件格式），SequenceFile里面存储多个文件，存储形式为文件路径+名称为key，文件内容为value。</p>
</li>
</ol>
</li>
<li><p>步骤：</p>
<ol>
<li><p>自定义一个类继承FileInputFormat</p>
<ol>
<li>重写isSplitable()方法，返回false，不可切割</li>
<li>重写createRecordReader()，创建自定义的RecordReader对象，并初始化</li>
</ol>
</li>
<li><p>改写RecordReader，实现一次读取一个完整的文件封装成KV</p>
<ol>
<li>采用IO流一次读取一个文件输出到value中，因为设置了不可分片，最终把所有的文件都封装到了value中</li>
<li>获取文件路径信息+名称，并设置key</li>
</ol>
</li>
<li><p>设置Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置输入的inputFormat</span></span><br><span class="line">job.setInputFormatClass(xxx.class);</span><br><span class="line"><span class="comment">// 设置输出的outputFormat</span></span><br><span class="line">job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/inputformat" target="_blank" rel="noopener">完整代码路径</a></p>
</li>
</ol>
</li>
</ol>
<h3 id="3-6-2-MapReduce-工作流程"><a href="#3-6-2-MapReduce-工作流程" class="headerlink" title="3.6.2. MapReduce 工作流程"></a>3.6.2. MapReduce 工作流程</h3><p>map阶段</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/mapworkflow.png" alt></p>
<p>reduce阶段：</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/reduceworkflow.png" alt></p>
<h3 id="3-6-3-Shuffle机制"><a href="#3-6-3-Shuffle机制" class="headerlink" title="3.6.3. Shuffle机制"></a>3.6.3. Shuffle机制</h3><p>Map方法之后Reduce之前的数据处理过程称为Shuffle机制。</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/ShuffleWorkFlow.png" alt></p>
<h4 id="3-6-3-1-Partition分区"><a href="#3-6-3-1-Partition分区" class="headerlink" title="3.6.3.1. Partition分区"></a>3.6.3.1. Partition分区</h4><p>将结果分别存入不同的文件（分区）中。</p>
<ol>
<li><p>默认分区</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashPartitioner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (key.hashCode() &amp; <span class="number">2147483647</span>) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>默认分区根据key的hashcode对ReduceTasks个数取模得到。用户无法控制哪个key存储到哪个分区。</p>
</li>
<li><p>自定义分区步骤</p>
<ol>
<li><p>自定义类继承Partitioner，重写getPartitioner()方法</p>
</li>
<li><p>在job中设置自定义Partitioner</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义Partitioner后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(n);</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h4 id="3-6-3-2-自定义Partitioner案例"><a href="#3-6-3-2-自定义Partitioner案例" class="headerlink" title="3.6.3.2. 自定义Partitioner案例"></a>3.6.3.2. 自定义Partitioner案例</h4><p>需求：  根据手机号的前3为进行分文件存储</p>
<ol>
<li><p>自定义分区类</p>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/blob/master/Hadoop/hdfs/src/main/java/mapreduce/serial/ProvincePartitioner.java" target="_blank" rel="noopener">代码路径</a></p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FLowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FLowBean fLowBean, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// text是手机号</span></span><br><span class="line">        <span class="comment">// fLowBean 流量信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取前3位</span></span><br><span class="line">        String preNum = text.toString().substring(<span class="number">0</span>,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"136"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>修改driver</p>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/blob/master/Hadoop/hdfs/src/main/java/mapreduce/serial/FlowDriver.java" target="_blank" rel="noopener">代码路径</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置分区类,及分区个数</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /output</span><br><span class="line">Found 6 items</span><br><span class="line">-rw-r--r--   3 bruce supergroup          0 2020-11-19 15:52 /output/_SUCCESS</span><br><span class="line">-rw-r--r--   3 bruce supergroup      15648 2020-11-19 15:52 /output/part-r-00000</span><br><span class="line">-rw-r--r--   3 bruce supergroup      21877 2020-11-19 15:52 /output/part-r-00001</span><br><span class="line">-rw-r--r--   3 bruce supergroup      20247 2020-11-19 15:52 /output/part-r-00002</span><br><span class="line">-rw-r--r--   3 bruce supergroup      39426 2020-11-19 15:52 /output/part-r-00003</span><br><span class="line">-rw-r--r--   3 bruce supergroup     286408 2020-11-19 15:52 /output/part-r-00004</span><br></pre></td></tr></table></figure>
</li>
<li><p>总结：</p>
<ol>
<li>如果<code>setNumReduceTasks()</code>数量大于<code>getPartition</code>中设置的数量，则会产生几个空的输出文件</li>
<li>如果 1&lt;<code>setNumReduceTasks()</code>&lt;<code>getPartition</code>中设置的数量，则会报异常</li>
<li>如果<code>setNumReduceTasks()</code>数量=1，则只会产生一个文件</li>
<li>分区号必须从0开始</li>
</ol>
</li>
</ol>
<h4 id="3-6-3-3-WritableComparable-排序"><a href="#3-6-3-3-WritableComparable-排序" class="headerlink" title="3.6.3.3. WritableComparable 排序"></a>3.6.3.3. WritableComparable 排序</h4><p>MapTask和ReduceTask均会对数据按照key进行排序。任何应用程序中的数据均会被排序，不管逻辑上是否需要。</p>
<p>默认排序是按照字典顺序排序，实现方式是快排。</p>
<p>对于MapTask，他会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值（80%）后，在对缓冲区中的数据进行一次快排，并将数据溢写到磁盘上，数据处理完后，它将磁盘上的所有文件进行归并排序。 </p>
<p>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过阈值，则溢写到磁盘否则存储在内存中。如果磁盘文件数目达到一定阈值，则进行一次归并排序生成一个更大的文件；如果内存中问价大小或数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完后，ReduceTask统一对磁盘上的所有数据进行一次归并排序。</p>
<ol>
<li><p>排序分类</p>
<ol>
<li><p>部分排序</p>
<p>MapReduce根据记录的键对数据集排序。保证输出的每个文件内部有序</p>
</li>
<li><p>全排序</p>
<p>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。</p>
</li>
<li><p>辅助排序：（GroupingComparator分组）</p>
<p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入同一个reduce方法时，可采用分组排序。</p>
</li>
<li><p>二次排序</p>
<p>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p>
</li>
</ol>
</li>
<li><p>自定义WritableComparable步骤</p>
<ol>
<li>bean对象作为key传输，实现WritableComparable接口，重写compareTo方法。</li>
</ol>
</li>
</ol>
<h4 id="3-6-3-4-自定义WritableComparable-排序案例（全排序）"><a href="#3-6-3-4-自定义WritableComparable-排序案例（全排序）" class="headerlink" title="3.6.3.4. 自定义WritableComparable 排序案例（全排序）"></a>3.6.3.4. 自定义WritableComparable 排序案例（全排序）</h4><p><a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/sort" target="_blank" rel="noopener">完整代码路径</a></p>
<ol>
<li><p>定义bean</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    ....</span><br><span class="line">    <span class="comment">//其他部分与序列化的代码相同，主要的不同是实现了其他接口，并实现compareTo方法</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (sum &gt; bean.getSum()) &#123;</span><br><span class="line">            res = -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sum &lt; bean.getSum()) &#123;</span><br><span class="line">            res = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   .</span><br><span class="line">   .</span><br><span class="line">   .</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] items = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 3 封装对象</span></span><br><span class="line">        String phoneNum = items[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">long</span> up = Long.parseLong(items[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">long</span> down = Long.parseLong(items[<span class="number">2</span>]);</span><br><span class="line">        flowBean.set(up, down);</span><br><span class="line"></span><br><span class="line">        v.set(phoneNum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 写出</span></span><br><span class="line">        context.write(flowBean, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现reducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Text value: values</span><br><span class="line">             ) &#123;</span><br><span class="line">            context.write(value, key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="3-6-3-5-自定义WritableComparable-排序案例（分区排序）"><a href="#3-6-3-5-自定义WritableComparable-排序案例（分区排序）" class="headerlink" title="3.6.3.5. 自定义WritableComparable 排序案例（分区排序）"></a>3.6.3.5. 自定义WritableComparable 排序案例（分区排序）</h4><p>在上一节的代码中添加分区的代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean bean, Text text, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line">        String preNum = text.toString().substring(<span class="number">0</span>,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"136"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修改driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<h4 id="3-6-3-6-Combiner-合并"><a href="#3-6-3-6-Combiner-合并" class="headerlink" title="3.6.3.6. Combiner 合并"></a>3.6.3.6. Combiner 合并</h4><ol>
<li><p>Combiner是MR程序中Mapper和Reducer之外的一种组件</p>
</li>
<li><p>Combiner的父类是Reducer</p>
</li>
<li><p>Combiner和Reducer的区别在于运行的位置：</p>
<p>Combiner是在每一个MapTask所在的节点运行</p>
<p>Reducer是接收全局所有Mapper的输出结果</p>
</li>
<li><p>Combiner的意义在于每一个MapTask的输出进行局部汇总，以减小网络传输量</p>
</li>
<li><p>Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟Reducer的输入kv类型要对应起来</p>
</li>
</ol>
<h4 id="3-6-3-7-Combiner-案例"><a href="#3-6-3-7-Combiner-案例" class="headerlink" title="3.6.3.7. Combiner 案例"></a>3.6.3.7. Combiner 案例</h4><ol>
<li><p>需求</p>
<p>统计过程中对每一个MapTask的输出进行局部汇总，以减少网络传输量。</p>
</li>
</ol>
<p>方案一： 自定义Combiner</p>
<ol>
<li><p><a href="https://github.com/bruceEeZhao/JavaLearn/blob/master/Hadoop/hdfs/src/main/java/mapreduce/wordcount/WordCountCombiner.java" target="_blank" rel="noopener">自定义Combiner</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value: values</span><br><span class="line">             ) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在driver中关联Combiner</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Combiner</span></span><br><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Map input records=8</span><br><span class="line">Map output records=12</span><br><span class="line">Map output bytes=116</span><br><span class="line">Map output materialized bytes=87</span><br><span class="line">Input split bytes=101</span><br><span class="line">Combine input records=12</span><br><span class="line">Combine output records=7</span><br><span class="line">Reduce input groups=7</span><br><span class="line">Reduce shuffle bytes=87</span><br><span class="line">Reduce input records=7</span><br><span class="line">Reduce output records=7</span><br></pre></td></tr></table></figure>

<p>Combiner将Map的输出减少到7</p>
</li>
</ol>
<p>方案二：</p>
<p>直接将Reducer关联为Combiner，因为做的操作是相同的，所以这里可以这么用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// Combiner</span><br><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Map input records=8</span><br><span class="line">Map output records=12</span><br><span class="line">Map output bytes=116</span><br><span class="line">Map output materialized bytes=87</span><br><span class="line">Input split bytes=101</span><br><span class="line">Combine input records=12</span><br><span class="line">Combine output records=7</span><br><span class="line">Reduce input groups=7</span><br></pre></td></tr></table></figure>

<h4 id="3-6-3-8-GroupingComparator"><a href="#3-6-3-8-GroupingComparator" class="headerlink" title="3.6.3.8. GroupingComparator"></a>3.6.3.8. GroupingComparator</h4><ol>
<li><p>需求</p>
<p>求出每个订单中最贵的商品</p>
</li>
<li><p>思路</p>
<ol>
<li>利用”订单id“和成交金额作为key，可以将Map阶段读取到的所有订单数据按id升序，如果id相同在按照金额降序排序，发送到Reducer</li>
<li>在Reducer端利用GroupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵的商品</li>
</ol>
</li>
<li><p>分组排序步骤：</p>
<ol>
<li><p>自定义类继承writableCompatator</p>
</li>
<li><p>重写compare方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">super</span>.compare(a, b);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
</li>
</ol>
<ol start="3">
<li><p>创建一个构造将比较对象的类传给父类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="4">
<li><p>案例</p>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/order" target="_blank" rel="noopener">完整代码路径</a></p>
<ol>
<li><p>bean对象</p>
<p>这里的排序，是先根据id进行升序排序，如果价格相同，则按照价格降序排序</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> order_id;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> price;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 先按照id升序，如果相同，按价格降序</span></span><br><span class="line">        <span class="keyword">if</span> (order_id &gt; o.getOrder_id()) &#123;</span><br><span class="line">            res = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order_id &lt; o.getOrder_id()) &#123;</span><br><span class="line">            res = -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (price &gt; o.getPrice()) &#123;</span><br><span class="line">                res = -<span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(price &lt; o.getPrice()) &#123;</span><br><span class="line">                res = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeInt(order_id);</span><br><span class="line">        dataOutput.writeDouble(price);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        order_id = dataInput.readInt();</span><br><span class="line">        price = dataInput.readDouble();</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="comment">// ..... 不重要的部分删去了</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
</li>
</ol>
<ol start="2">
<li><p>Mapper</p>
<p>Mapper中将数据从文件中读出，然后将字符串中的编号和价格拿出来，组成一个bean对象，由于不需要使用value，因此value传了一个NullWritable对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    OrderBean bean = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取一行 // 000001 Pdt_01 222.8</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 切割</span></span><br><span class="line">        String[] items = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 为bean赋值</span></span><br><span class="line">        bean.set(Integer.parseInt(items[<span class="number">0</span>]), Double.parseDouble(items[<span class="number">2</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(bean, NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reducer</p>
<p>Reducer写出key和每个key中的第一个value，这里由于是把id+价格作为key，因此将同id不同价格的记录全部打印了，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>GroupingComparator</p>
<p>根据第三条的问题，我们需要添加一个GroupingComparator。它做的事情是把id相同的记录处理成一个key，这样之后Reducer再处理的时候就不会有问题了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 只要id相同，就认为是相同的key</span></span><br><span class="line">        OrderBean beana = (OrderBean) a;</span><br><span class="line">        OrderBean beanb = (OrderBean) b;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (beana.getOrder_id() &gt; beanb.getOrder_id()) &#123;</span><br><span class="line">            res = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (beana.getOrder_id() &lt; beanb.getOrder_id()) &#123;</span><br><span class="line">            res = -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
</li>
</ol>
<h3 id="3-6-4-MapTask工作机制"><a href="#3-6-4-MapTask工作机制" class="headerlink" title="3.6.4. MapTask工作机制"></a>3.6.4. MapTask工作机制</h3><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/MapTaskWorkFlow.png" alt></p>
<h3 id="3-6-5-ReduceTask工作机制"><a href="#3-6-5-ReduceTask工作机制" class="headerlink" title="3.6.5. ReduceTask工作机制"></a>3.6.5. ReduceTask工作机制</h3><ol>
<li><p>工作机制</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/ReduceTaskWorkFlow.png" alt></p>
</li>
<li><p>设置ReduceTask并行度（个数）</p>
<p>与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以手动设置的</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认是1，可以手动设置</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意事项</p>
<ol>
<li><p>ReduceTask=0，表示没有Reduce阶段，输出文件个数与Map个数一致</p>
</li>
<li><p>ReduceTask默认值是1，所以输出文件个数为一个</p>
</li>
<li><p>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜</p>
</li>
<li><p>ReduceTask数量，需要考虑业务逻辑，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask</p>
</li>
<li><p>具体多少个ReduceTask，需根据集群性能而定</p>
</li>
<li><p>如果分区数不是1，但ReduceTask是1，是否执行分区过程？不执行。</p>
<p>因为在MapTask源码中，执行分区的前提是判断ReduceNum个数是否大于1，不大于1不执行</p>
</li>
</ol>
</li>
</ol>
<h3 id="3-6-6-OutputFormat数据输出"><a href="#3-6-6-OutputFormat数据输出" class="headerlink" title="3.6.6. OutputFormat数据输出"></a>3.6.6. OutputFormat数据输出</h3><h4 id="3-6-6-1-OutputFormat接口实现类"><a href="#3-6-6-1-OutputFormat接口实现类" class="headerlink" title="3.6.6.1. OutputFormat接口实现类"></a>3.6.6.1. OutputFormat接口实现类</h4><p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。</p>
<ol>
<li><p>文本输出TextOutputFormat</p>
<p>默认的输出是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用的是toString方法，把他们转换为字符串。</p>
</li>
<li><p>SequenceFileOutPutFormat</p>
<p>将SequenceFileOutPutFormat输出作为后续MapReduce任务的输入，它的格式紧凑，很容易被压缩。</p>
</li>
<li><p>自定义OutputFormat</p>
</li>
</ol>
<h4 id="3-6-6-2-自定义OutputFormat"><a href="#3-6-6-2-自定义OutputFormat" class="headerlink" title="3.6.6.2. 自定义OutputFormat"></a>3.6.6.2. 自定义OutputFormat</h4><ol>
<li><p>使用场景</p>
<p>为了实现控制问价的输出路径和输出格式，可以自定义OutputFormat</p>
<p>例如：要在MapReduce程序中根据数据的不同输出两类结果袋不同目录，这类灵活的输出需求可以通过自定义OutPutFormat来实现</p>
</li>
<li><p>自定义OutputFormat步骤</p>
<ol>
<li>自定义一个类继承FileOutPutFormat</li>
<li>改写RecordWriter，具体改写输出数据的方法write()</li>
<li>在驱动类中进行关联</li>
</ol>
</li>
</ol>
<p>案例：</p>
<p>​    需求：过滤日志，将包含baidu.com的日志输出到baidu.log, 将其他的日志输出到other.log</p>
<p>​    <a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/outputformat" target="_blank" rel="noopener">完整代码路径</a></p>
<ol>
<li><p>mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// http://www.baidu.com</span></span><br><span class="line"></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>reducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (NullWritable v: values</span><br><span class="line">             ) &#123;</span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>OutPutFormat</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FRecordWriter(job);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Writer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    FSDataOutputStream fosbaidu;</span><br><span class="line">    FSDataOutputStream fosother;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 1 获取文件系统</span></span><br><span class="line">            FileSystem fs = FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2 创建输出到baidu.log的输出流</span></span><br><span class="line">            fosbaidu = fs.create(<span class="keyword">new</span> Path(<span class="string">"/baidu.log"</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3 创建输出到other.log的输出流</span></span><br><span class="line">            fosother = fs.create(<span class="keyword">new</span> Path(<span class="string">"/other.log"</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text text, NullWritable nullWritable)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 判断key中是否有baidu，如果有，写出到baidu.log否则，写出到other.log</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (text.toString().contains(<span class="string">"baidu"</span>)) &#123;</span><br><span class="line">            fosbaidu.write(text.toString().getBytes());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            fosother.write(text.toString().getBytes());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        IOUtils.closeStream(fosbaidu);</span><br><span class="line">        IOUtils.closeStream(fosother);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        <span class="comment">// 1 获取Job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar存储位置</span></span><br><span class="line">        job.setJarByClass(FilterDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联Map和Reduce类</span></span><br><span class="line">        job.setMapperClass(FilterMapper.class);</span><br><span class="line">        job.setReducerClass(FilterReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置Mapper阶段输出数据的key，value类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key，value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关联自定义的输出格式类</span></span><br><span class="line">        job.setOutputFormatClass(FilterOutputFormat.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/log"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 虽然自定义了OutPutFormat，但是因为OutPutFormat继承自FileOutPutFormat</span></span><br><span class="line">        <span class="comment">// 而FileOutPutFormat要输出一个_SUCCESS文件，所以这里还需要指定一个目录。</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/output"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h3 id="3-6-7-Join的多种应用"><a href="#3-6-7-Join的多种应用" class="headerlink" title="3.6.7. Join的多种应用"></a>3.6.7. Join的多种应用</h3><h4 id="3-6-7-1-Reduce-Join"><a href="#3-6-7-1-Reduce-Join" class="headerlink" title="3.6.7.1. Reduce Join"></a>3.6.7.1. Reduce Join</h4><p>Reduce Join工作原理：</p>
<p>Map端主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。</p>
<p>Reduce端的主要工作：以连接字段作为key的分组以完成，我们只需要在每一个分组中将来自不同文件的记录（Map阶段已经打标签）。最后合并就ok了。</p>
<h4 id="3-6-7-2-Reduce-Join-案例"><a href="#3-6-7-2-Reduce-Join-案例" class="headerlink" title="3.6.7.2. Reduce Join 案例"></a>3.6.7.2. Reduce Join 案例</h4><p>需求：</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/join_table.png" alt></p>
<p>将两张表join</p>
<ol>
<li><p>Map需要处理的事情</p>
<ol>
<li>获取输入文件类型</li>
<li>获取输入数据</li>
<li>不同文件分别处理</li>
<li>封装Bean对象输出</li>
</ol>
</li>
<li><p>Reduce</p>
<ol>
<li>Reduce方法缓存订单数据集合和产品表，然后合并</li>
</ol>
</li>
</ol>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/join" target="_blank" rel="noopener">完整代码路径</a></p>
<ol>
<li><p>bean</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// id pid amount</span></span><br><span class="line">    <span class="comment">// pid pname</span></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line">    <span class="keyword">private</span> String pid;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> amount;</span><br><span class="line">    <span class="keyword">private</span> String pname;</span><br><span class="line">    <span class="keyword">private</span> String flag;  <span class="comment">//标记是哪个表</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TableBean</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(id);</span><br><span class="line">        dataOutput.writeUTF(pid);</span><br><span class="line">        dataOutput.writeInt(amount);</span><br><span class="line">        dataOutput.writeUTF(pname);</span><br><span class="line">        dataOutput.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        id = dataInput.readUTF();</span><br><span class="line">        pid = dataInput.readUTF();</span><br><span class="line">        amount = dataInput.readInt();</span><br><span class="line">        pname = dataInput.readUTF();</span><br><span class="line">        flag = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略部分代码</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">" "</span> +  amount + <span class="string">" "</span> + pname ;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line">    String name;</span><br><span class="line">    TableBean bean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取文件名称</span></span><br><span class="line">        FileSplit split = (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line">        name = split.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// id pid amount</span></span><br><span class="line">        <span class="comment">// 1001 01 1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// pid pname</span></span><br><span class="line">        <span class="comment">// 01 小米</span></span><br><span class="line">        <span class="comment">// 02 华为</span></span><br><span class="line">        <span class="comment">// 03 格力</span></span><br><span class="line">        Text k = <span class="keyword">new</span> Text();</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] items = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (name.equals(<span class="string">"amount_table"</span>)) &#123; <span class="comment">// amount</span></span><br><span class="line">            bean.setId(items[<span class="number">0</span>]);</span><br><span class="line">            bean.setPid(items[<span class="number">1</span>]);</span><br><span class="line">            bean.setAmount(Integer.parseInt(items[<span class="number">2</span>]));</span><br><span class="line">            bean.setPname(<span class="string">""</span>);</span><br><span class="line">            bean.setFlag(<span class="string">"amount"</span>);</span><br><span class="line">            k.set(items[<span class="number">1</span>]);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// company</span></span><br><span class="line">            bean.setId(<span class="string">""</span>);</span><br><span class="line">            bean.setPid(items[<span class="number">0</span>]);</span><br><span class="line">            bean.setAmount(<span class="number">0</span>);</span><br><span class="line">            bean.setPname(items[<span class="number">1</span>]);</span><br><span class="line">            bean.setFlag(<span class="string">"company"</span>);</span><br><span class="line">            k.set(items[<span class="number">0</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(k, bean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 存储产品数量结合</span></span><br><span class="line">        ArrayList&lt;TableBean&gt; beans = <span class="keyword">new</span> ArrayList&lt;TableBean&gt;();</span><br><span class="line">        <span class="comment">// 存放公司集合</span></span><br><span class="line">        TableBean company = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean bean:values</span><br><span class="line">             ) &#123;</span><br><span class="line">            <span class="keyword">if</span> (bean.getFlag().equals(<span class="string">"amount"</span>)) &#123;</span><br><span class="line">                TableBean dst = <span class="keyword">new</span> TableBean();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(dst, bean);</span><br><span class="line">                    beans.add(dst);</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(company, bean);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean bean: beans</span><br><span class="line">             ) &#123;</span><br><span class="line">            bean.setPname(company.getPname());</span><br><span class="line">            context.write(bean, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -cat /output5/part-r-00000</span><br><span class="line">1004 4 小米</span><br><span class="line">1001 1 小米</span><br><span class="line">1005 5 华为</span><br><span class="line">1002 2 华为</span><br><span class="line">1006 6 格力</span><br><span class="line">1003 3 格力</span><br></pre></td></tr></table></figure>
</li>
<li><p>缺点</p>
<p>在这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。</p>
<p>解决方案： Map端实现数据合并。</p>
</li>
</ol>
<h4 id="3-6-7-3-Map-Join"><a href="#3-6-7-3-Map-Join" class="headerlink" title="3.6.7.3. Map Join"></a>3.6.7.3. Map Join</h4><ol>
<li><p>使用场景</p>
<p>适用于有一张小表（可放入内存中），一张大表的情况。</p>
</li>
<li><p>方法：采用 DistributedCache</p>
<ol>
<li><p>在Mapper的setup阶段，将文件读取到缓存集合中</p>
</li>
<li><p>在驱动函数中加载缓存</p>
<p>// 缓存普通文件到Task运行节点</p>
<p>job.addCacheFile(new URI(“file://sss”));</p>
</li>
</ol>
</li>
</ol>
<p>需要注意的事情</p>
<ol>
<li><p>DistributedCacheDriver缓存文件</p>
<ol>
<li><p>加载缓存数据： job.addCacheFile(new URI(“file://sss”));</p>
</li>
<li><p>Map端join的逻辑不需要Reduce阶段，设置ReduceTask数量为0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(0);</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>读取缓存的文件数据：</p>
<p>setup()方法处理</p>
</li>
</ol>
<h4 id="3-6-7-4-Map-Join-案例"><a href="#3-6-7-4-Map-Join-案例" class="headerlink" title="3.6.7.4. Map Join 案例"></a>3.6.7.4. Map Join 案例</h4><p><a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/cache" target="_blank" rel="noopener">完整代码路径</a></p>
<ol>
<li><p>Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    HashMap&lt;String, String&gt;  cpMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 缓存小表</span></span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(<span class="string">"company_table"</span>), <span class="string">"UTF-8"</span>));</span><br><span class="line"></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine())) &#123;</span><br><span class="line">            <span class="comment">// pid pname</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 1. 切割</span></span><br><span class="line">            String[] items = line.split(<span class="string">" "</span>);</span><br><span class="line">            cpMap.put(items[<span class="number">0</span>], items[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// id pid amount</span></span><br><span class="line">        <span class="comment">// pid pname</span></span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] items = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取pid</span></span><br><span class="line">        String pid = items[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从map中取值</span></span><br><span class="line">        String pidName = cpMap.get(pid);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拼接</span></span><br><span class="line">        line = line + <span class="string">" "</span> + pidName;</span><br><span class="line">        k.set(line);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(k, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        <span class="comment">// 1 获取Job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar存储位置</span></span><br><span class="line">        job.setJarByClass(DistributedDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联Map和Reduce类</span></span><br><span class="line">        job.setMapperClass(DistributedCacheMapper.class);</span><br><span class="line">        <span class="comment">//job.setReducerClass(TableReducer.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 没有reduce阶段了</span></span><br><span class="line"><span class="comment">//        // 4 设置Mapper阶段输出数据的key，value类型</span></span><br><span class="line"><span class="comment">//        job.setMapOutputKeyClass(Text.class);</span></span><br><span class="line"><span class="comment">//        job.setMapOutputValueClass(TableBean.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key，value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载缓存数据</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> Path(<span class="string">"/join/company_table"</span>).toUri());</span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/join2"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/output6"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -cat /output6/part-m-00000</span><br><span class="line">1001 01 1 小米</span><br><span class="line">1002 02 2 华为</span><br><span class="line">1003 03 3 格力</span><br><span class="line">1004 01 4 小米</span><br><span class="line">1005 02 5 华为</span><br><span class="line">1006 03 6 格力</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h3 id="3-6-8-数据清洗（ELT）"><a href="#3-6-8-数据清洗（ELT）" class="headerlink" title="3.6.8. 数据清洗（ELT）"></a>3.6.8. 数据清洗（ELT）</h3><p>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清洗掉不符合要求的数据。清晰的过程只需运行Mapper程序，不需要运行Reduce程序。</p>
<p><a href="https://github.com/bruceEeZhao/JavaLearn/tree/master/Hadoop/hdfs/src/main/java/mapreduce/logelt" target="_blank" rel="noopener">代码完整路径</a></p>
<ol>
<li><p>Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 读取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 解析数据</span></span><br><span class="line">        <span class="keyword">boolean</span> result = parseLog(line, context);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!result) &#123;</span><br><span class="line">            <span class="comment">// 不符合要求，直接返回</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line">        String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">            <span class="comment">// 计数器，为了自定义一些打印输出</span></span><br><span class="line">            context.getCounter(<span class="string">"map"</span>, <span class="string">"true"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 计数器</span></span><br><span class="line">            context.getCounter(<span class="string">"map"</span>, <span class="string">"false"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line">        <span class="comment">// 1 获取Job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar存储位置</span></span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联Map和Reduce类</span></span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        <span class="comment">//job.setReducerClass(TableReducer.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 没有reduce阶段了</span></span><br><span class="line"><span class="comment">//        // 4 设置Mapper阶段输出数据的key，value类型</span></span><br><span class="line"><span class="comment">//        job.setMapOutputKeyClass(Text.class);</span></span><br><span class="line"><span class="comment">//        job.setMapOutputValueClass(TableBean.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key，value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/logelt"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/output7"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>控制台输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// Mapper中添加的计数器</span><br><span class="line">....</span><br><span class="line">map</span><br><span class="line">		false=5</span><br><span class="line">		true=25</span><br><span class="line">....</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h1 id="4-Hadoop-数据压缩"><a href="#4-Hadoop-数据压缩" class="headerlink" title="4. Hadoop 数据压缩"></a>4. Hadoop 数据压缩</h1><p>压缩技术可以有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘的效率。在运行MR程序时，I/O操作，网络数据传输，Shuffle和Merge要花大量时间，尤其是数据规模很大和工作负载密集的情况下，因此数据压缩就显得十分重要。</p>
<p>压缩使用的基本原则：</p>
<ol>
<li>运算密集型的job，少用压缩</li>
<li>IO密集型的job，多用压缩</li>
</ol>
<h2 id="4-1-MR支持的压缩格式"><a href="#4-1-MR支持的压缩格式" class="headerlink" title="4.1. MR支持的压缩格式"></a>4.1. MR支持的压缩格式</h2><table>
<thead>
<tr>
<th>压缩格式</th>
<th>Hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFAULT</td>
<td>是</td>
<td>DEFAULT</td>
<td>.default</td>
<td>否</td>
<td>不需要</td>
</tr>
<tr>
<td>Gzip</td>
<td>是</td>
<td>DEFAULT</td>
<td>.gz</td>
<td>否</td>
<td>不需要</td>
</tr>
<tr>
<td>bzip2</td>
<td>是</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>不需要</td>
</tr>
<tr>
<td>LZO</td>
<td>否</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建立索引格式，还需指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>不需要</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编/解码器：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>编/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>EFAULT</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>org.apache.hadoop.io.compress.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3G</td>
<td>1.8G</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3G</td>
<td>1.1G</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3G</td>
<td>2.9G</td>
<td>49.4MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<p>Snappy:</p>
<blockquote>
<p>Compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
</blockquote>
<h2 id="4-2-压缩位置的选择"><a href="#4-2-压缩位置的选择" class="headerlink" title="4.2. 压缩位置的选择"></a>4.2. 压缩位置的选择</h2><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/compress_location.png" alt></p>
<h2 id="4-3-压缩参数设置"><a href="#4-3-压缩参数设置" class="headerlink" title="4.3. 压缩参数设置"></a>4.3. 压缩参数设置</h2><table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs<br>在core-site.xml中配置</td>
<td>org.apache.hadoop.io.compress.DefaultCodec<br>org.apache.hadoop.io.compress.GzipCodec<br>org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入阶段</td>
<td>Hadoop使用文件扩展名<br>判断是否支持某种编码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress<br>在mapred-site.xml中配置</td>
<td>false</td>
<td>mapper输出</td>
<td>参数设置为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec<br>在mapred-site.xml中配置</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy在此阶段</td>
</tr>
<tr>
<td>maperduce.output.fileoutputformat.<br>compress<br>在mapred-site.xml中配置</td>
<td>false</td>
<td>reducer输出</td>
<td>参数设置为true启用压缩</td>
</tr>
<tr>
<td>maperduce.output.fileoutputformat.<br>compress.codec<br>在mapred-site.xml中配置</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具编解码，如gizp和bzip2</td>
</tr>
<tr>
<td>maperduce.output.fileoutputformat.<br>compress.type<br>在mapred-site.xml中配置</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：None和Block</td>
</tr>
</tbody></table>
<h2 id="4-4-数据流的压缩-解压缩"><a href="#4-4-数据流的压缩-解压缩" class="headerlink" title="4.4. 数据流的压缩/解压缩"></a>4.4. 数据流的压缩/解压缩</h2><p>CompressionCodec有两个方法可以实现压缩/解压缩</p>
<p>压缩： createOutputStream(OutputStreamout)</p>
<p>解压： createInputStream(InputStreamin)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestCompress</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">        <span class="comment">// compress("/home/bruce/Desktop/mapreduce/log", "org.apache.hadoop.io.compress.BZip2Codec");</span></span><br><span class="line">        <span class="comment">// compress("/home/bruce/Desktop/mapreduce/log", "org.apache.hadoop.io.compress.GzipCodec");</span></span><br><span class="line"></span><br><span class="line">        decompress(<span class="string">"/home/bruce/Desktop/mapreduce/log.gz"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String fileName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 压缩方式检查</span></span><br><span class="line">        CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line">        CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(fileName));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (codec == <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"can not access"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 获取输入流</span></span><br><span class="line">        FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(fileName));</span><br><span class="line">        CompressionInputStream cis = codec.createInputStream(fis);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 获取输出流</span></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(fileName+<span class="string">".decode"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 流的对拷</span></span><br><span class="line">        IOUtils.copyBytes(cis, fos, <span class="number">1024</span>*<span class="number">1024</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">        IOUtils.closeStream(cis);</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String fileName, String method)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取输入流</span></span><br><span class="line">        FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(fileName));</span><br><span class="line"></span><br><span class="line">        Class classCodec = Class.forName(method);</span><br><span class="line">        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(classCodec, <span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取输出流</span></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(fileName+codec.getDefaultExtension()));</span><br><span class="line"></span><br><span class="line">        CompressionOutputStream cos = codec.createOutputStream(fos);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 流的对拷</span></span><br><span class="line">        IOUtils.copyBytes(fis, cos, <span class="number">1024</span>*<span class="number">1024</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(cos);</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">        IOUtils.closeStream(fos);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-4-1-Map输出端压缩"><a href="#4-4-1-Map输出端压缩" class="headerlink" title="4.4.1. Map输出端压缩"></a>4.4.1. Map输出端压缩</h3><p>在Driver中添加</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">      conf.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</span><br><span class="line">      <span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">      conf.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec.class, CompressionCodec.class);</span><br></pre></td></tr></table></figure>

<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 bruce supergroup          0 2020-11-26 19:46 /output/_SUCCESS</span><br><span class="line">-rw-r--r--   3 bruce supergroup         53 2020-11-26 19:46 /output/part-r-00000</span><br></pre></td></tr></table></figure>

<p>Map输出端进行压缩，不会影响最终的输出</p>
<h3 id="4-4-2-Reduce输出端压缩"><a href="#4-4-2-Reduce输出端压缩" class="headerlink" title="4.4.2. Reduce输出端压缩"></a>4.4.2. Reduce输出端压缩</h3><p>在Driver中添加</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置Reducer端输出压缩开启</span></span><br><span class="line">     FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 设置压缩方式</span></span><br><span class="line">     FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br></pre></td></tr></table></figure>

<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 bruce supergroup          0 2020-11-26 19:51 /output/_SUCCESS</span><br><span class="line">-rw-r--r--   3 bruce supergroup         76 2020-11-26 19:51 /output/part-r-00000.bz2</span><br></pre></td></tr></table></figure>

<h1 id="5-Yarn资源调度器"><a href="#5-Yarn资源调度器" class="headerlink" title="5. Yarn资源调度器"></a>5. Yarn资源调度器</h1><p>Yarn是一个资源调度器，负责为运算程序提供服务器运算资源。</p>
<h2 id="5-1-基本架构"><a href="#5-1-基本架构" class="headerlink" title="5.1. 基本架构"></a>5.1. 基本架构</h2><p>Yarn主要由ResourceManager， NodeManager， ApplicationMaster和Container的组件构成。</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/yarn_architecture.gif" alt></p>
<h2 id="5-2-Yarn工作机制"><a href="#5-2-Yarn工作机制" class="headerlink" title="5.2. Yarn工作机制"></a>5.2. Yarn工作机制</h2><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/yarnWorkFlow.png" alt></p>
<h2 id="5-3-资源调度器"><a href="#5-3-资源调度器" class="headerlink" title="5.3. 资源调度器"></a>5.3. 资源调度器</h2><p>Hadoop作业调度器主要有三种： FIFO，Capacity Scheduler，和Fair Scheduler。Hadoop2.7.2 默认调度器是Capacity Scheduler</p>
<ol>
<li><p>FIFO</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/fifo.png" alt></p>
</li>
<li><p>Capacity Scheduler(容量调度器)</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/capacity.png" alt></p>
</li>
<li><p>Fair Scheduler</p>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/fair.png" alt></p>
</li>
</ol>
<h2 id="5-4-任务的推测执行"><a href="#5-4-任务的推测执行" class="headerlink" title="5.4. 任务的推测执行"></a>5.4. 任务的推测执行</h2><ol>
<li><p>作业的完成时间取决于最慢的任务完成时间</p>
<p>一个作业由若干个Map任务和Reduce任务构成，因硬件老化等原因，某些任务可能运行十分缓慢。</p>
</li>
<li><p>推测执行机制</p>
<p>发现拖后腿任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，采用谁的结果。</p>
</li>
<li><p>执行推测任务的前提条件</p>
<ol>
<li><p>每个Task只能有一个备份任务</p>
</li>
<li><p>当前job已经完成的Task必须不小于0.05（%5）</p>
</li>
<li><p>开启推测执行参数设置。mapred-site.xml文件中是默认打开的。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>不能启用推测执行机制的情况</p>
<ol>
<li>任务间存在严重的负载倾斜</li>
<li>特殊任务，比如任务向数据库中写数据</li>
</ol>
</li>
</ol>
<h2 id="5-5-推测执行算法原理"><a href="#5-5-推测执行算法原理" class="headerlink" title="5.5. 推测执行算法原理"></a>5.5. 推测执行算法原理</h2><p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/speculate.png" alt></p>
<h1 id="6-Hadoop企业优化"><a href="#6-Hadoop企业优化" class="headerlink" title="6. Hadoop企业优化"></a>6. Hadoop企业优化</h1><h2 id="6-1-MapReduce跑的慢的原因"><a href="#6-1-MapReduce跑的慢的原因" class="headerlink" title="6.1. MapReduce跑的慢的原因"></a>6.1. MapReduce跑的慢的原因</h2><p>MapReduce效率瓶颈在于两点：</p>
<ol>
<li><p>计算机性能</p>
<p>CPU，内存，磁盘，网络</p>
</li>
<li><p>IO操作优化</p>
<ol>
<li>数据倾斜</li>
<li>Map和Reduce数设置不合理</li>
<li>Map运行时间太长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量不可分块的超大文件</li>
<li>溢写次数过多</li>
<li>Merge次数过多</li>
</ol>
</li>
</ol>
<h2 id="6-2-MapReduce优化方法"><a href="#6-2-MapReduce优化方法" class="headerlink" title="6.2. MapReduce优化方法"></a>6.2. MapReduce优化方法</h2><p>优化方法主要从六个方面考虑：数据输入，Map阶段，Reduce阶段，IO传输，数据倾斜问题和常用的调优参数。</p>
<h3 id="6-2-1-数据输入"><a href="#6-2-1-数据输入" class="headerlink" title="6.2.1. 数据输入"></a>6.2.1. 数据输入</h3><ol>
<li>合并小文件：在执行任务前将小文件进行合并，大量的小文件会产生大量的Map任务。</li>
<li>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。</li>
</ol>
<h3 id="6-2-2-Map阶段"><a href="#6-2-2-Map阶段" class="headerlink" title="6.2.2. Map阶段"></a>6.2.2. Map阶段</h3><ol>
<li>减少溢写次数：通过调整io.sort.mb（环形缓冲区的大小）及sort.spill.percent参数，增大出发spill的内存上限，减少spill次数，从而减少磁盘IO</li>
<li>减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge次数</li>
<li>在Map之后，在不影响业务逻辑的前提下，先进行combine处理，减少IO</li>
</ol>
<h3 id="6-2-3-Reduce阶段"><a href="#6-2-3-Reduce阶段" class="headerlink" title="6.2.3. Reduce阶段"></a>6.2.3. Reduce阶段</h3><ol>
<li>设置合理Map和Reduce数</li>
<li>设置Map，Reduce共存：调整slowstart.completedmaps参数，是Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</li>
<li>规避使用Reduce：因为Reduce在用于连接数据集的时候会产生大量的网络消耗</li>
<li>合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据会写入磁盘，然后Reduce会从磁盘中获得所有的数据。可以通过参数设置，使得Buffer中的一部分数据可以直接输送到Reduce，从而能减少IO开销：<code>mapred.job.reduce.input.buffer.percent</code>默认为0.0.当该值大于0时，会保留指定比例的内存读Buffer中的数据直接给Reduce使用。</li>
</ol>
<h3 id="6-2-4-IO传输"><a href="#6-2-4-IO传输" class="headerlink" title="6.2.4. IO传输"></a>6.2.4. IO传输</h3><ol>
<li>采用数据压缩的方式：减少网络IO的时间。（LZO，Snappy）</li>
<li>使用SequenceFile二进制文件</li>
</ol>
<h3 id="6-2-5-数据倾斜"><a href="#6-2-5-数据倾斜" class="headerlink" title="6.2.5. 数据倾斜"></a>6.2.5. 数据倾斜</h3><ol>
<li><p>数据倾斜</p>
<ol>
<li>频率倾斜：某一区域的数据量远大于其他区域</li>
<li>大小倾斜：部分记录的大小远大于平均值</li>
</ol>
</li>
<li><p>解决方法</p>
<ol>
<li><p>抽样和范围分区</p>
<p>通过对原始数据进行抽样得到的结果集来预设分区边界值。</p>
</li>
<li><p>自定义分区</p>
</li>
<li><p>Combine</p>
</li>
<li><p>采用Map Join，尽量避免Reduce Join</p>
</li>
</ol>
</li>
</ol>
<h3 id="6-2-6-常用调优参数"><a href="#6-2-6-常用调优参数" class="headerlink" title="6.2.6. 常用调优参数"></a>6.2.6. 常用调优参数</h3><h2 id="6-3-小文件优化方法"><a href="#6-3-小文件优化方法" class="headerlink" title="6.3. 小文件优化方法"></a>6.3. 小文件优化方法</h2><p>HDFS上每个文件都需要在NameNode上建立索引，每个索引大小约为150byte</p>
<h2 id="6-3-1-解决方法"><a href="#6-3-1-解决方法" class="headerlink" title="6.3.1. 解决方法"></a>6.3.1. 解决方法</h2><ol>
<li>在数据采集时，将小文件或小批数据合并后再上传</li>
<li>在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并</li>
<li>在MapReduce处理时，可采用CombineTextInputFormat提高效率</li>
</ol>
<ol>
<li><p>Hadoop Archive</p>
<p>文件归档成HAR文件</p>
</li>
<li><p>Sequence File</p>
<p>由一系列的二进制KV组成</p>
</li>
<li><p>CombineTextInputFormat</p>
<p>将多个文件合并成一个Split，另外，他会考虑数据的存储位置</p>
</li>
<li><p>开启JVM重用</p>
<p>对于大量的小文件job，可以开启JVM重用，会减少45%运行时间</p>
<p>原理： 一个Map运行在JVM上，开启重用，该Map在JVM上运行完毕后，JVM继续运行其他Map（类似线程池）</p>
<p>设置： mapreduce.job.jvm.numtasks值在10-20之间</p>
</li>
</ol>
<h1 id="7-扩展案例"><a href="#7-扩展案例" class="headerlink" title="7. 扩展案例"></a>7. 扩展案例</h1><h2 id="7-1-倒序索引（多job串联）"><a href="#7-1-倒序索引（多job串联）" class="headerlink" title="7.1. 倒序索引（多job串联）"></a>7.1. 倒序索引（多job串联）</h2><p>多job串联即多个MapReduce任务，下一个任务使用上一个任务的结果</p>
<ol>
<li>输入数据及预期结果</li>
</ol>
<p><img src="//bruceeezhao.github.io/2020/11/05/Hadoop/7_1.png" alt></p>
<ol start="2">
<li><p>Mapper1</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneIndexMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    String name;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取文件名称</span></span><br><span class="line">        FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">        name = inputSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : fields</span><br><span class="line">             ) &#123;</span><br><span class="line">            k.set(word+<span class="string">"--"</span>+name);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reducer1</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneIndexReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    IntWritable value = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable v: values</span><br><span class="line">             ) &#123;</span><br><span class="line">            sum += v.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        value.set(sum);</span><br><span class="line"></span><br><span class="line">        context.write(key, value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver1</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneIndexDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取Job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar存储位置</span></span><br><span class="line">        job.setJarByClass(OneIndexDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联Map和Reduce类</span></span><br><span class="line">        job.setMapperClass(OneIndexMapper.class);</span><br><span class="line">        job.setReducerClass(OneIndexReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置Mapper阶段输出数据的key，value类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key，value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/index"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/output1"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Mapper2</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoIndexMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//        ads--a	3</span></span><br><span class="line"><span class="comment">//        ads--b	2</span></span><br><span class="line"><span class="comment">//        ads--c	1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">"--"</span>);</span><br><span class="line"></span><br><span class="line">        k.set(fields[<span class="number">0</span>]);</span><br><span class="line">        v.set(fields[<span class="number">1</span>]);</span><br><span class="line">        context.write(k, v);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reducer2</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoIndexReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        StringBuffer tmp = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">        <span class="keyword">for</span> (Text value: values</span><br><span class="line">             ) &#123;</span><br><span class="line">            tmp.append(value.toString().replace(<span class="string">"\t"</span>, <span class="string">"--&gt;"</span>)+<span class="string">"\t"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(tmp.toString());</span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver2</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoIndexDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9091"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取Job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar存储位置</span></span><br><span class="line">        job.setJarByClass(TwoIndexDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联Map和Reduce类</span></span><br><span class="line">        job.setMapperClass(TwoIndexMapper.class);</span><br><span class="line">        job.setReducerClass(TwoIndexReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置Mapper阶段输出数据的key，value类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key，value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/index2"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/output2"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/大数据/" rel="tag"># 大数据</a>
              <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/11/01/Java多线程/" rel="prev" title="Java多线程">
      <i class="fa fa-chevron-left"></i> Java多线程
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/11/06/设计模式学习之-04工厂模式/" rel="next" title="设计模式学习之-04工厂模式">
      设计模式学习之-04工厂模式 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Hadoop初识"><span class="nav-number">1.</span> <span class="nav-text">1. Hadoop初识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Hadoop1-x-与-2-x的区别"><span class="nav-number">1.1.</span> <span class="nav-text">1.1. Hadoop1.x 与 2.x的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-HDFS架构"><span class="nav-number">1.2.</span> <span class="nav-text">1.2. HDFS架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Yarn架构"><span class="nav-number">1.3.</span> <span class="nav-text">1.3. Yarn架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-MapReduce"><span class="nav-number">1.4.</span> <span class="nav-text">1.4. MapReduce</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-HDFS"><span class="nav-number">2.</span> <span class="nav-text">2. HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-优缺点"><span class="nav-number">2.1.</span> <span class="nav-text">2.1. 优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-优点"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1. 优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-缺点"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2. 缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-组成架构"><span class="nav-number">2.2.</span> <span class="nav-text">2.2. 组成架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-文件块大小（面试题）"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1. 文件块大小（面试题）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-HDFS-API"><span class="nav-number">2.3.</span> <span class="nav-text">2.3. HDFS API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-环境配置"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1. 环境配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-上传文件"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2. 上传文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-下载文件"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.3. 下载文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-4-文件删除"><span class="nav-number">2.3.4.</span> <span class="nav-text">2.3.4. 文件删除</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-5-文件信息打印"><span class="nav-number">2.3.5.</span> <span class="nav-text">2.3.5. 文件信息打印</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-HDFS-I-O流操作"><span class="nav-number">2.4.</span> <span class="nav-text">2.4. HDFS I/O流操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-上传文件"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.4.1. 上传文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-下载文件"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2. 下载文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-3-读取部分文件"><span class="nav-number">2.4.3.</span> <span class="nav-text">2.4.3. 读取部分文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-HDFS数据流"><span class="nav-number">2.5.</span> <span class="nav-text">2.5. HDFS数据流</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-1-写入流程"><span class="nav-number">2.5.1.</span> <span class="nav-text">2.5.1. 写入流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-1-1-数据写入"><span class="nav-number">2.5.1.1.</span> <span class="nav-text">2.5.1.1. 数据写入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-1-2-副本存储节点选择"><span class="nav-number">2.5.1.2.</span> <span class="nav-text">2.5.1.2. 副本存储节点选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-1-3-节点距离的计算"><span class="nav-number">2.5.1.3.</span> <span class="nav-text">2.5.1.3. 节点距离的计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-2-读取流程"><span class="nav-number">2.5.2.</span> <span class="nav-text">2.5.2. 读取流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-NameNode和Secondary-NameNode"><span class="nav-number">2.6.</span> <span class="nav-text">2.6. NameNode和Secondary NameNode</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-1-工作机制"><span class="nav-number">2.6.1.</span> <span class="nav-text">2.6.1. 工作机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-2-Fsimage-和-Edits"><span class="nav-number">2.6.2.</span> <span class="nav-text">2.6.2. Fsimage 和 Edits</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-3-NameNode多目录配置"><span class="nav-number">2.6.3.</span> <span class="nav-text">2.6.3. NameNode多目录配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-Datanode（面试重点）"><span class="nav-number">2.7.</span> <span class="nav-text">2.7. Datanode（面试重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-1-工作机制"><span class="nav-number">2.7.1.</span> <span class="nav-text">2.7.1. 工作机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-2-数据完整性"><span class="nav-number">2.7.2.</span> <span class="nav-text">2.7.2. 数据完整性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-3-白名单-amp-黑名单"><span class="nav-number">2.7.3.</span> <span class="nav-text">2.7.3. 白名单&amp;黑名单</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-4-Datanode多目录配置"><span class="nav-number">2.7.4.</span> <span class="nav-text">2.7.4. Datanode多目录配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8-HDFS-2-X新特性"><span class="nav-number">2.8.</span> <span class="nav-text">2.8. HDFS 2.X新特性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-1-集群将数据拷贝"><span class="nav-number">2.8.1.</span> <span class="nav-text">2.8.1. 集群将数据拷贝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-2-小文件存档"><span class="nav-number">2.8.2.</span> <span class="nav-text">2.8.2 小文件存档</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-3-回收站"><span class="nav-number">2.8.3.</span> <span class="nav-text">2.8.3. 回收站</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-4-快照"><span class="nav-number">2.8.4.</span> <span class="nav-text">2.8.4. 快照</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-MapReduce"><span class="nav-number">3.</span> <span class="nav-text">3. MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-优缺点"><span class="nav-number">3.1.</span> <span class="nav-text">3.1. 优缺点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-编程思想"><span class="nav-number">3.2.</span> <span class="nav-text">3.2. 编程思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-编程规范"><span class="nav-number">3.3.</span> <span class="nav-text">3.3. 编程规范</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-WordCount-案例"><span class="nav-number">3.4.</span> <span class="nav-text">3.4. WordCount 案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-序列化"><span class="nav-number">3.5.</span> <span class="nav-text">3.5. 序列化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-1-定义"><span class="nav-number">3.5.1.</span> <span class="nav-text">3.5.1. 定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-2-为什么不用Java的序列化"><span class="nav-number">3.5.2.</span> <span class="nav-text">3.5.2. 为什么不用Java的序列化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-3-序列化流程"><span class="nav-number">3.5.3.</span> <span class="nav-text">3.5.3. 序列化流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-4-案例"><span class="nav-number">3.5.4.</span> <span class="nav-text">3.5.4. 案例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-MapReduce框架原理"><span class="nav-number">3.6.</span> <span class="nav-text">3.6. MapReduce框架原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-1-InputFormat数据输入"><span class="nav-number">3.6.1.</span> <span class="nav-text">3.6.1. InputFormat数据输入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-1-1-切片与MapTask并行度决定机制"><span class="nav-number">3.6.1.1.</span> <span class="nav-text">3.6.1.1. 切片与MapTask并行度决定机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-1-2-Job提交流程和切片源码"><span class="nav-number">3.6.1.2.</span> <span class="nav-text">3.6.1.2. Job提交流程和切片源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-1-3-CombineTextInputFormat-切片"><span class="nav-number">3.6.1.3.</span> <span class="nav-text">3.6.1.3. CombineTextInputFormat 切片</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-1-4-FileInputFormat实现类"><span class="nav-number">3.6.1.4.</span> <span class="nav-text">3.6.1.4. FileInputFormat实现类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-1-5-自定义InputFormat"><span class="nav-number">3.6.1.5.</span> <span class="nav-text">3.6.1.5. 自定义InputFormat</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-2-MapReduce-工作流程"><span class="nav-number">3.6.2.</span> <span class="nav-text">3.6.2. MapReduce 工作流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-3-Shuffle机制"><span class="nav-number">3.6.3.</span> <span class="nav-text">3.6.3. Shuffle机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-1-Partition分区"><span class="nav-number">3.6.3.1.</span> <span class="nav-text">3.6.3.1. Partition分区</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-2-自定义Partitioner案例"><span class="nav-number">3.6.3.2.</span> <span class="nav-text">3.6.3.2. 自定义Partitioner案例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-3-WritableComparable-排序"><span class="nav-number">3.6.3.3.</span> <span class="nav-text">3.6.3.3. WritableComparable 排序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-4-自定义WritableComparable-排序案例（全排序）"><span class="nav-number">3.6.3.4.</span> <span class="nav-text">3.6.3.4. 自定义WritableComparable 排序案例（全排序）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-5-自定义WritableComparable-排序案例（分区排序）"><span class="nav-number">3.6.3.5.</span> <span class="nav-text">3.6.3.5. 自定义WritableComparable 排序案例（分区排序）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-6-Combiner-合并"><span class="nav-number">3.6.3.6.</span> <span class="nav-text">3.6.3.6. Combiner 合并</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-7-Combiner-案例"><span class="nav-number">3.6.3.7.</span> <span class="nav-text">3.6.3.7. Combiner 案例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-8-GroupingComparator"><span class="nav-number">3.6.3.8.</span> <span class="nav-text">3.6.3.8. GroupingComparator</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-4-MapTask工作机制"><span class="nav-number">3.6.4.</span> <span class="nav-text">3.6.4. MapTask工作机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-5-ReduceTask工作机制"><span class="nav-number">3.6.5.</span> <span class="nav-text">3.6.5. ReduceTask工作机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-6-OutputFormat数据输出"><span class="nav-number">3.6.6.</span> <span class="nav-text">3.6.6. OutputFormat数据输出</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-6-1-OutputFormat接口实现类"><span class="nav-number">3.6.6.1.</span> <span class="nav-text">3.6.6.1. OutputFormat接口实现类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-6-2-自定义OutputFormat"><span class="nav-number">3.6.6.2.</span> <span class="nav-text">3.6.6.2. 自定义OutputFormat</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-7-Join的多种应用"><span class="nav-number">3.6.7.</span> <span class="nav-text">3.6.7. Join的多种应用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-7-1-Reduce-Join"><span class="nav-number">3.6.7.1.</span> <span class="nav-text">3.6.7.1. Reduce Join</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-7-2-Reduce-Join-案例"><span class="nav-number">3.6.7.2.</span> <span class="nav-text">3.6.7.2. Reduce Join 案例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-7-3-Map-Join"><span class="nav-number">3.6.7.3.</span> <span class="nav-text">3.6.7.3. Map Join</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-7-4-Map-Join-案例"><span class="nav-number">3.6.7.4.</span> <span class="nav-text">3.6.7.4. Map Join 案例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-8-数据清洗（ELT）"><span class="nav-number">3.6.8.</span> <span class="nav-text">3.6.8. 数据清洗（ELT）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Hadoop-数据压缩"><span class="nav-number">4.</span> <span class="nav-text">4. Hadoop 数据压缩</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-MR支持的压缩格式"><span class="nav-number">4.1.</span> <span class="nav-text">4.1. MR支持的压缩格式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-压缩位置的选择"><span class="nav-number">4.2.</span> <span class="nav-text">4.2. 压缩位置的选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-压缩参数设置"><span class="nav-number">4.3.</span> <span class="nav-text">4.3. 压缩参数设置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-数据流的压缩-解压缩"><span class="nav-number">4.4.</span> <span class="nav-text">4.4. 数据流的压缩/解压缩</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-1-Map输出端压缩"><span class="nav-number">4.4.1.</span> <span class="nav-text">4.4.1. Map输出端压缩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-2-Reduce输出端压缩"><span class="nav-number">4.4.2.</span> <span class="nav-text">4.4.2. Reduce输出端压缩</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Yarn资源调度器"><span class="nav-number">5.</span> <span class="nav-text">5. Yarn资源调度器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-基本架构"><span class="nav-number">5.1.</span> <span class="nav-text">5.1. 基本架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Yarn工作机制"><span class="nav-number">5.2.</span> <span class="nav-text">5.2. Yarn工作机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-资源调度器"><span class="nav-number">5.3.</span> <span class="nav-text">5.3. 资源调度器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-任务的推测执行"><span class="nav-number">5.4.</span> <span class="nav-text">5.4. 任务的推测执行</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-推测执行算法原理"><span class="nav-number">5.5.</span> <span class="nav-text">5.5. 推测执行算法原理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Hadoop企业优化"><span class="nav-number">6.</span> <span class="nav-text">6. Hadoop企业优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-MapReduce跑的慢的原因"><span class="nav-number">6.1.</span> <span class="nav-text">6.1. MapReduce跑的慢的原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-MapReduce优化方法"><span class="nav-number">6.2.</span> <span class="nav-text">6.2. MapReduce优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-1-数据输入"><span class="nav-number">6.2.1.</span> <span class="nav-text">6.2.1. 数据输入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-2-Map阶段"><span class="nav-number">6.2.2.</span> <span class="nav-text">6.2.2. Map阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-3-Reduce阶段"><span class="nav-number">6.2.3.</span> <span class="nav-text">6.2.3. Reduce阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-4-IO传输"><span class="nav-number">6.2.4.</span> <span class="nav-text">6.2.4. IO传输</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-5-数据倾斜"><span class="nav-number">6.2.5.</span> <span class="nav-text">6.2.5. 数据倾斜</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-6-常用调优参数"><span class="nav-number">6.2.6.</span> <span class="nav-text">6.2.6. 常用调优参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-小文件优化方法"><span class="nav-number">6.3.</span> <span class="nav-text">6.3. 小文件优化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-1-解决方法"><span class="nav-number">6.4.</span> <span class="nav-text">6.3.1. 解决方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-扩展案例"><span class="nav-number">7.</span> <span class="nav-text">7. 扩展案例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-倒序索引（多job串联）"><span class="nav-number">7.1.</span> <span class="nav-text">7.1. 倒序索引（多job串联）</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bruce zhao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/bruceEeZhao" title="GitHub → https://github.com/bruceEeZhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/bruceezhao@foxmail.com" title="E-Mail → bruceezhao@foxmail.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bruce zhao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.5.0
  </div>

        








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  
















  

  

</body>
</html>
